{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- ЭТАП 1: Загрузка и первичная обработка ---\")\n",
        "\n",
        "# --- 1.1: Определяем пути к файлам (замени на свои, если нужно) ---\n",
        "# Убедись, что файлы лежат там, откуда Colab может их прочитать\n",
        "# (например, загружены в сессию или лежат на подключенном Google Drive)\n",
        "fires_file = 'fires.csv'\n",
        "supplies_file = 'supplies.csv'\n",
        "temperature_file = 'temperature.csv'\n",
        "weather_files = ['weather_data_2019.csv', 'weather_data_2020.csv'] # Файлы погоды\n",
        "\n",
        "# --- 1.2: Обработка fires.csv ---\n",
        "print(f\"\\nОбработка {fires_file}...\")\n",
        "df_fires_a1 = pd.DataFrame() # Инициализируем пустым на случай ошибки\n",
        "try:\n",
        "    if not os.path.exists(fires_file):\n",
        "        raise FileNotFoundError(f\"Файл {fires_file} не найден.\")\n",
        "    df_fires = pd.read_csv(fires_file)\n",
        "    print(f\" Исходный размер: {df_fires.shape}\")\n",
        "    df_fires_a1 = df_fires[df_fires['Груз'] == 'A1'].copy()\n",
        "    print(f\" Размер после фильтрации по 'A1': {df_fires_a1.shape}\")\n",
        "except FileNotFoundError as fnf_error:\n",
        "    print(f\" Ошибка: {fnf_error}\")\n",
        "except Exception as e:\n",
        "    print(f\" Ошибка при обработке {fires_file}: {e}\")\n",
        "\n",
        "# --- 1.3: Обработка supplies.csv ---\n",
        "print(f\"\\nОбработка {supplies_file}...\")\n",
        "df_supplies_a1 = pd.DataFrame()\n",
        "try:\n",
        "    if not os.path.exists(supplies_file):\n",
        "        raise FileNotFoundError(f\"Файл {supplies_file} не найден.\")\n",
        "    df_supplies = pd.read_csv(supplies_file)\n",
        "    print(f\" Исходный размер: {df_supplies.shape}\")\n",
        "    df_supplies_a1 = df_supplies[df_supplies['Наим. ЕТСНГ'] == 'A1'].copy()\n",
        "    print(f\" Размер после фильтрации по 'A1': {df_supplies_a1.shape}\")\n",
        "except FileNotFoundError as fnf_error:\n",
        "    print(f\" Ошибка: {fnf_error}\")\n",
        "except Exception as e:\n",
        "    print(f\" Ошибка при обработке {supplies_file}: {e}\")\n",
        "\n",
        "# --- 1.4: Обработка temperature.csv ---\n",
        "print(f\"\\nОбработка {temperature_file}...\")\n",
        "df_temp_a1 = pd.DataFrame()\n",
        "try:\n",
        "    if not os.path.exists(temperature_file):\n",
        "        raise FileNotFoundError(f\"Файл {temperature_file} не найден.\")\n",
        "    df_temp = pd.read_csv(temperature_file)\n",
        "    print(f\" Исходный размер: {df_temp.shape}\")\n",
        "    cols_to_drop_temp = ['Пикет', 'Смена']\n",
        "    existing_cols_temp = [col for col in cols_to_drop_temp if col in df_temp.columns]\n",
        "    if existing_cols_temp:\n",
        "        df_temp.drop(columns=existing_cols_temp, inplace=True)\n",
        "        print(f\" Удалены колонки: {existing_cols_temp}\")\n",
        "    df_temp['Марка'] = df_temp['Марка'].astype(str).apply(lambda x: 'A1' if x.strip().startswith('A1') else x)\n",
        "    print(\" Марка нормализована.\")\n",
        "    df_temp_a1 = df_temp[df_temp['Марка'] == 'A1'].copy()\n",
        "    print(f\" Размер после фильтрации по 'A1': {df_temp_a1.shape}\")\n",
        "except FileNotFoundError as fnf_error:\n",
        "    print(f\" Ошибка: {fnf_error}\")\n",
        "except Exception as e:\n",
        "    print(f\" Ошибка при обработке {temperature_file}: {e}\")\n",
        "\n",
        "# --- 1.5: Обработка и объединение файлов погоды ---\n",
        "print(f\"\\nОбработка файлов погоды: {weather_files}...\")\n",
        "all_weather_dfs = []\n",
        "df_weather_combined = pd.DataFrame()\n",
        "cols_to_drop_weather = ['wind_dir', 'v_max', 'v_avg']\n",
        "for weather_file in weather_files:\n",
        "    print(f\" Обработка файла: {weather_file}\")\n",
        "    try:\n",
        "        if not os.path.exists(weather_file):\n",
        "            raise FileNotFoundError(f\"Файл {weather_file} не найден.\")\n",
        "        df_weather_single = pd.read_csv(weather_file)\n",
        "        print(f\"  Исходный размер: {df_weather_single.shape}\")\n",
        "        existing_cols_weather = [col for col in cols_to_drop_weather if col in df_weather_single.columns]\n",
        "        if existing_cols_weather:\n",
        "            df_weather_single.drop(columns=existing_cols_weather, inplace=True)\n",
        "            print(f\"  Удалены колонки: {existing_cols_weather}\")\n",
        "        all_weather_dfs.append(df_weather_single)\n",
        "    except FileNotFoundError as fnf_error:\n",
        "        print(f\"  Ошибка: {fnf_error}. Файл пропущен.\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Ошибка при обработке {weather_file}: {e}. Файл пропущен.\")\n",
        "\n",
        "if all_weather_dfs:\n",
        "    df_weather_combined = pd.concat(all_weather_dfs, ignore_index=True)\n",
        "    print(f\"\\nДанные погоды за {len(all_weather_dfs)} год(а) объединены.\")\n",
        "    print(f\" Общий размер данных погоды: {df_weather_combined.shape}\")\n",
        "else:\n",
        "    print(\"\\nНе удалось загрузить ни одного файла погоды. DataFrame погоды пуст.\")\n",
        "\n",
        "print(\"\\n--- ЭТАП 1 ЗАВЕРШЕН ---\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- ЭТАП 2: Преобразование типов и анализ пропусков ---\n",
        "print(\"\\n--- ЭТАП 2: Преобразование типов и анализ пропусков ---\")\n",
        "\n",
        "# Функция для безопасного преобразования в datetime\n",
        "def safe_to_datetime(series, **kwargs):\n",
        "    converted = pd.to_datetime(series, errors='coerce', **kwargs)\n",
        "    return converted\n",
        "\n",
        "# Функция для безопасного преобразования в числовой тип\n",
        "def safe_to_numeric(series):\n",
        "    if pd.api.types.is_string_dtype(series):\n",
        "        if series.str.contains(',').any():\n",
        "             # print(f\"  Замена ',' на '.' в столбце '{series.name}'\") # Раскомментируй для отладки\n",
        "             series = series.str.replace(',', '.', regex=False)\n",
        "    return pd.to_numeric(series, errors='coerce')\n",
        "\n",
        "# 2.1 Обработка df_fires_a1\n",
        "print(\"\\nОбработка типов данных в df_fires_a1...\")\n",
        "if not df_fires_a1.empty:\n",
        "    date_cols_fires = ['Дата составления', 'Дата начала', 'Дата оконч.', 'Нач. форм. штабеля']\n",
        "    for col in date_cols_fires:\n",
        "        if col in df_fires_a1.columns and not pd.api.types.is_datetime64_any_dtype(df_fires_a1[col]):\n",
        "            print(f\" Преобразование '{col}' в datetime...\")\n",
        "            df_fires_a1[col] = safe_to_datetime(df_fires_a1[col])\n",
        "    num_cols_fires = ['Вес по акту, тн']\n",
        "    for col in num_cols_fires:\n",
        "        if col in df_fires_a1.columns and not pd.api.types.is_numeric_dtype(df_fires_a1[col]):\n",
        "            print(f\" Преобразование '{col}' в numeric...\")\n",
        "            df_fires_a1[col] = safe_to_numeric(df_fires_a1[col])\n",
        "    print(\"Типы данных df_fires_a1 после преобразования:\")\n",
        "    df_fires_a1.info()\n",
        "else:\n",
        "    print(\"DataFrame df_fires_a1 пуст или не был создан на этапе 1.\")\n",
        "\n",
        "# 2.2 Обработка df_supplies_a1\n",
        "print(\"\\nОбработка типов данных в df_supplies_a1...\")\n",
        "if not df_supplies_a1.empty:\n",
        "    date_cols_supplies = ['ВыгрузкаНаСклад', 'ПогрузкаНаСудно']\n",
        "    for col in date_cols_supplies:\n",
        "         if col in df_supplies_a1.columns and not pd.api.types.is_datetime64_any_dtype(df_supplies_a1[col]):\n",
        "            print(f\" Преобразование '{col}' в datetime...\")\n",
        "            df_supplies_a1[col] = safe_to_datetime(df_supplies_a1[col])\n",
        "    num_cols_supplies = ['На склад, тн', 'На судно, тн']\n",
        "    for col in num_cols_supplies:\n",
        "        if col in df_supplies_a1.columns and not pd.api.types.is_numeric_dtype(df_supplies_a1[col]):\n",
        "            print(f\" Преобразование '{col}' в numeric...\")\n",
        "            df_supplies_a1[col] = safe_to_numeric(df_supplies_a1[col])\n",
        "    print(\"\\nТипы данных df_supplies_a1 после преобразования:\")\n",
        "    df_supplies_a1.info()\n",
        "else:\n",
        "    print(\"DataFrame df_supplies_a1 пуст или не был создан на этапе 1.\")\n",
        "\n",
        "# 2.3 Обработка df_temp_a1\n",
        "print(\"\\nОбработка типов данных в df_temp_a1...\")\n",
        "if not df_temp_a1.empty:\n",
        "    date_cols_temp = ['Дата акта']\n",
        "    for col in date_cols_temp:\n",
        "        if col in df_temp_a1.columns and not pd.api.types.is_datetime64_any_dtype(df_temp_a1[col]):\n",
        "            print(f\" Преобразование '{col}' в datetime...\")\n",
        "            df_temp_a1[col] = safe_to_datetime(df_temp_a1[col])\n",
        "    num_cols_temp = ['Максимальная температура']\n",
        "    for col in num_cols_temp:\n",
        "         if col in df_temp_a1.columns and not pd.api.types.is_numeric_dtype(df_temp_a1[col]):\n",
        "            print(f\" Преобразование '{col}' в numeric...\")\n",
        "            df_temp_a1[col] = safe_to_numeric(df_temp_a1[col])\n",
        "    print(\"\\nТипы данных df_temp_a1 после преобразования:\")\n",
        "    df_temp_a1.info()\n",
        "else:\n",
        "    print(\"DataFrame df_temp_a1 пуст или не был создан на этапе 1.\")\n",
        "\n",
        "# 2.4 Обработка df_weather_combined\n",
        "print(\"\\nОбработка типов данных в df_weather_combined...\")\n",
        "if not df_weather_combined.empty:\n",
        "    date_cols_weather = ['date']\n",
        "    for col in date_cols_weather:\n",
        "        if col in df_weather_combined.columns and not pd.api.types.is_datetime64_any_dtype(df_weather_combined[col]):\n",
        "            print(f\" Преобразование '{col}' в datetime...\")\n",
        "            df_weather_combined[col] = safe_to_datetime(df_weather_combined[col])\n",
        "    num_cols_weather = ['t', 'p', 'humidity', 'precipitation', 'cloudcover', 'visibility', 'weather_code']\n",
        "    for col in num_cols_weather:\n",
        "        if col in df_weather_combined.columns and not pd.api.types.is_numeric_dtype(df_weather_combined[col]):\n",
        "             print(f\" Преобразование '{col}' в numeric...\")\n",
        "             df_weather_combined[col] = safe_to_numeric(df_weather_combined[col])\n",
        "    print(\"\\nТипы данных df_weather_combined после преобразования:\")\n",
        "    df_weather_combined.info()\n",
        "else:\n",
        "    print(\"DataFrame df_weather_combined пуст или не был создан на этапе 1.\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- 2.5 Анализ пропусков ---\n",
        "print(\"\\n--- Анализ пропущенных значений ---\")\n",
        "\n",
        "def analyze_missing_values(df, df_name):\n",
        "    if df is None or df.empty:\n",
        "        print(f\"\\nDataFrame '{df_name}' пуст или не существует, анализ пропусков невозможен.\")\n",
        "        return\n",
        "    print(f\"\\nАнализ пропусков для '{df_name}':\")\n",
        "    missing_count = df.isnull().sum()\n",
        "    missing_percent = (df.isnull().mean() * 100).round(2)\n",
        "    missing_df = pd.DataFrame({'Количество пропусков': missing_count, 'Процент пропусков (%)': missing_percent})\n",
        "    missing_df = missing_df[missing_df['Количество пропусков'] > 0]\n",
        "    if not missing_df.empty:\n",
        "        print(missing_df.sort_values(by='Количество пропусков', ascending=False))\n",
        "    else:\n",
        "        print(\"Пропущенные значения отсутствуют.\")\n",
        "\n",
        "analyze_missing_values(df_fires_a1 if 'df_fires_a1' in locals() else None, \"df_fires_a1\")\n",
        "analyze_missing_values(df_supplies_a1 if 'df_supplies_a1' in locals() else None, \"df_supplies_a1\")\n",
        "analyze_missing_values(df_temp_a1 if 'df_temp_a1' in locals() else None, \"df_temp_a1\")\n",
        "analyze_missing_values(df_weather_combined if 'df_weather_combined' in locals() else None, \"df_weather_combined\")\n",
        "\n",
        "print(\"\\n--- ЭТАП 2 ЗАВЕРШЕН ---\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# --- ЭТАП 3: Сохранение (опционально) ---\n",
        "# Если нужно сохранить DataFrames с правильными типами для дальнейшего использования\n",
        "save_typed_files = True # Поставь True для сохранения\n",
        "\n",
        "if save_typed_files:\n",
        "    print(\"\\n--- ЭТАП 3: Сохранение файлов с преобразованными типами ---\")\n",
        "    output_dir_typed = \"processed_data_typed\"\n",
        "    if not os.path.exists(output_dir_typed):\n",
        "        os.makedirs(output_dir_typed)\n",
        "        print(f\"Создана директория: {output_dir_typed}\")\n",
        "    try:\n",
        "        if 'df_fires_a1' in locals() and not df_fires_a1.empty:\n",
        "            df_fires_a1.to_csv(os.path.join(output_dir_typed,'fires_A1_typed.csv'), index=False)\n",
        "        if 'df_supplies_a1' in locals() and not df_supplies_a1.empty:\n",
        "            df_supplies_a1.to_csv(os.path.join(output_dir_typed,'supplies_A1_typed.csv'), index=False)\n",
        "        if 'df_temp_a1' in locals() and not df_temp_a1.empty:\n",
        "            df_temp_a1.to_csv(os.path.join(output_dir_typed,'temperature_A1_typed.csv'), index=False)\n",
        "        if 'df_weather_combined' in locals() and not df_weather_combined.empty:\n",
        "            df_weather_combined.to_csv(os.path.join(output_dir_typed,'weather_2019_2020_typed.csv'), index=False)\n",
        "        print(f\"Файлы сохранены в папку: {output_dir_typed}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при сохранении файлов: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wcmdCKqqYW0",
        "outputId": "70c8ac05-3fb2-4381-c234-380b6952aedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ЭТАП 1: Загрузка и первичная обработка ---\n",
            "\n",
            "Обработка fires.csv...\n",
            " Ошибка: Файл fires.csv не найден.\n",
            "\n",
            "Обработка supplies.csv...\n",
            " Исходный размер: (6323, 7)\n",
            " Размер после фильтрации по 'A1': (4285, 7)\n",
            "\n",
            "Обработка temperature.csv...\n",
            " Исходный размер: (4106, 7)\n",
            " Удалены колонки: ['Пикет', 'Смена']\n",
            " Марка нормализована.\n",
            " Размер после фильтрации по 'A1': (4095, 5)\n",
            "\n",
            "Обработка файлов погоды: ['weather_data_2019.csv', 'weather_data_2020.csv']...\n",
            " Обработка файла: weather_data_2019.csv\n",
            "  Ошибка: Файл weather_data_2019.csv не найден.. Файл пропущен.\n",
            " Обработка файла: weather_data_2020.csv\n",
            "  Исходный размер: (8760, 11)\n",
            "  Удалены колонки: ['wind_dir', 'v_max', 'v_avg']\n",
            "\n",
            "Данные погоды за 1 год(а) объединены.\n",
            " Общий размер данных погоды: (8760, 8)\n",
            "\n",
            "--- ЭТАП 1 ЗАВЕРШЕН ---\n",
            "--------------------------------------------------\n",
            "\n",
            "--- ЭТАП 2: Преобразование типов и анализ пропусков ---\n",
            "\n",
            "Обработка типов данных в df_fires_a1...\n",
            "DataFrame df_fires_a1 пуст или не был создан на этапе 1.\n",
            "\n",
            "Обработка типов данных в df_supplies_a1...\n",
            " Преобразование 'ВыгрузкаНаСклад' в datetime...\n",
            " Преобразование 'ПогрузкаНаСудно' в datetime...\n",
            "\n",
            "Типы данных df_supplies_a1 после преобразования:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 4285 entries, 1 to 6322\n",
            "Data columns (total 7 columns):\n",
            " #   Column           Non-Null Count  Dtype         \n",
            "---  ------           --------------  -----         \n",
            " 0   ВыгрузкаНаСклад  4285 non-null   datetime64[ns]\n",
            " 1   Наим. ЕТСНГ      4285 non-null   object        \n",
            " 2   Штабель          4285 non-null   int64         \n",
            " 3   ПогрузкаНаСудно  4285 non-null   datetime64[ns]\n",
            " 4   На склад, тн     4285 non-null   float64       \n",
            " 5   На судно, тн     4283 non-null   float64       \n",
            " 6   Склад            4285 non-null   int64         \n",
            "dtypes: datetime64[ns](2), float64(2), int64(2), object(1)\n",
            "memory usage: 267.8+ KB\n",
            "\n",
            "Обработка типов данных в df_temp_a1...\n",
            " Преобразование 'Дата акта' в datetime...\n",
            "\n",
            "Типы данных df_temp_a1 после преобразования:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 4095 entries, 0 to 4105\n",
            "Data columns (total 5 columns):\n",
            " #   Column                    Non-Null Count  Dtype         \n",
            "---  ------                    --------------  -----         \n",
            " 0   Склад                     4095 non-null   int64         \n",
            " 1   Штабель                   4095 non-null   int64         \n",
            " 2   Марка                     4095 non-null   object        \n",
            " 3   Максимальная температура  4095 non-null   float64       \n",
            " 4   Дата акта                 4095 non-null   datetime64[ns]\n",
            "dtypes: datetime64[ns](1), float64(1), int64(2), object(1)\n",
            "memory usage: 192.0+ KB\n",
            "\n",
            "Обработка типов данных в df_weather_combined...\n",
            " Преобразование 'date' в datetime...\n",
            "\n",
            "Типы данных df_weather_combined после преобразования:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8760 entries, 0 to 8759\n",
            "Data columns (total 8 columns):\n",
            " #   Column         Non-Null Count  Dtype         \n",
            "---  ------         --------------  -----         \n",
            " 0   date           8760 non-null   datetime64[ns]\n",
            " 1   t              8760 non-null   float64       \n",
            " 2   p              8760 non-null   float64       \n",
            " 3   humidity       8760 non-null   int64         \n",
            " 4   precipitation  8760 non-null   float64       \n",
            " 5   cloudcover     8760 non-null   int64         \n",
            " 6   visibility     0 non-null      float64       \n",
            " 7   weather_code   8760 non-null   int64         \n",
            "dtypes: datetime64[ns](1), float64(4), int64(3)\n",
            "memory usage: 547.6 KB\n",
            "------------------------------\n",
            "\n",
            "--- Анализ пропущенных значений ---\n",
            "\n",
            "DataFrame 'df_fires_a1' пуст или не существует, анализ пропусков невозможен.\n",
            "\n",
            "Анализ пропусков для 'df_supplies_a1':\n",
            "              Количество пропусков  Процент пропусков (%)\n",
            "На судно, тн                     2                   0.05\n",
            "\n",
            "Анализ пропусков для 'df_temp_a1':\n",
            "Пропущенные значения отсутствуют.\n",
            "\n",
            "Анализ пропусков для 'df_weather_combined':\n",
            "            Количество пропусков  Процент пропусков (%)\n",
            "visibility                  8760                  100.0\n",
            "\n",
            "--- ЭТАП 2 ЗАВЕРШЕН ---\n",
            "--------------------------------------------------\n",
            "\n",
            "--- ЭТАП 3: Сохранение файлов с преобразованными типами ---\n",
            "Создана директория: processed_data_typed\n",
            "Файлы сохранены в папку: processed_data_typed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Шаг 3: Агрегация данных о погоде до дневного уровня ---\n",
        "print(\"\\n--- Шаг 3: Агрегация данных о погоде до дневного уровня ---\")\n",
        "\n",
        "if 'df_weather_combined' in locals() and not df_weather_combined.empty:\n",
        "    if 'date' in df_weather_combined.columns and pd.api.types.is_datetime64_any_dtype(df_weather_combined['date']):\n",
        "\n",
        "        # Убедимся, что 'date' - это индекс для удобной ресемплинг/агрегации\n",
        "        # Если дата еще не индекс:\n",
        "        if not isinstance(df_weather_combined.index, pd.DatetimeIndex):\n",
        "             print(\" Установка 'date' в качестве индекса...\")\n",
        "             # Сортируем по дате перед установкой индекса (важно для некоторых агрегаций)\n",
        "             df_weather_combined.sort_values('date', inplace=True)\n",
        "             df_weather_combined.set_index('date', inplace=True)\n",
        "\n",
        "        # Создаем столбец с датой без времени для группировки\n",
        "        df_weather_combined['day_date'] = df_weather_combined.index.date\n",
        "\n",
        "        print(\" Агрегация почасовых данных по дням...\")\n",
        "        # Определяем функции агрегации для каждого столбца\n",
        "        agg_functions = {\n",
        "            't': ['mean', 'min', 'max'], # Средняя, минимальная, максимальная температура за день\n",
        "            'p': ['mean', 'min', 'max'], # Давление\n",
        "            'humidity': ['mean', 'min'],  # Влажность (макс обычно 100, мин важнее)\n",
        "            'precipitation': 'sum',       # Сумма осадков за день\n",
        "            'cloudcover': 'mean',         # Средняя облачность\n",
        "            # 'weather_code': ['median', 'last'] # Медианный код погоды или последний за день (можно выбрать)\n",
        "            'weather_code': lambda x: x.mode()[0] if not x.mode().empty else np.nan # Самый частый код погоды за день\n",
        "        }\n",
        "\n",
        "        # Убедимся, что все колонки для агрегации существуют\n",
        "        valid_agg_functions = {k: v for k, v in agg_functions.items() if k in df_weather_combined.columns}\n",
        "        print(f\" Колонки для агрегации: {list(valid_agg_functions.keys())}\")\n",
        "\n",
        "        if valid_agg_functions:\n",
        "            # Группируем по дню и применяем агрегацию\n",
        "            df_weather_daily = df_weather_combined.groupby('day_date').agg(valid_agg_functions)\n",
        "\n",
        "            # Переименовываем столбцы для понятности (например, t_mean, t_min, ...)\n",
        "            df_weather_daily.columns = ['_'.join(col).strip() for col in df_weather_daily.columns.values]\n",
        "            # Исправляем имя для 'precipitation_sum' и 'weather_code_<lambda>'\n",
        "            df_weather_daily.rename(columns={'precipitation_sum': 'precip_total_day',\n",
        "                                             'weather_code_<lambda>':'weather_code_mode'}, inplace=True)\n",
        "\n",
        "\n",
        "            # Преобразуем индекс (который сейчас 'day_date' типа object) обратно в datetime\n",
        "            df_weather_daily.index = pd.to_datetime(df_weather_daily.index)\n",
        "\n",
        "            print(\"\\nПример агрегированных данных погоды (df_weather_daily):\")\n",
        "            print(df_weather_daily.head())\n",
        "            print(\"\\nРазмер агрегированных данных:\", df_weather_daily.shape)\n",
        "            df_weather_daily.info()\n",
        "\n",
        "        else:\n",
        "            print(\" Ошибка: Не найдены колонки для агрегации в df_weather_combined.\")\n",
        "            df_weather_daily = pd.DataFrame() # Создаем пустой на случай ошибки\n",
        "\n",
        "        # Удаляем вспомогательный столбец из исходного df_weather_combined (если он больше не нужен)\n",
        "        # df_weather_combined.drop(columns=['day_date'], inplace=True, errors='ignore')\n",
        "        # Можно вернуть индекс обратно в столбец, если нужно\n",
        "        # df_weather_combined.reset_index(inplace=True)\n",
        "\n",
        "    else:\n",
        "        print(\" Ошибка: Столбец 'date' не найден или не является datetime в df_weather_combined.\")\n",
        "        df_weather_daily = pd.DataFrame()\n",
        "else:\n",
        "    print(\" DataFrame df_weather_combined не найден или пуст. Агрегация погоды пропущена.\")\n",
        "    df_weather_daily = pd.DataFrame()\n",
        "\n",
        "print(\"\\n--- Шаг 3 ЗАВЕРШЕН ---\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVNj76_nqYT8",
        "outputId": "5c53303c-34f3-48dd-9855-c7fdd1fe577c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Шаг 3: Агрегация данных о погоде до дневного уровня ---\n",
            " Установка 'date' в качестве индекса...\n",
            " Агрегация почасовых данных по дням...\n",
            " Колонки для агрегации: ['t', 'p', 'humidity', 'precipitation', 'cloudcover', 'weather_code']\n",
            "\n",
            "Пример агрегированных данных погоды (df_weather_daily):\n",
            "              t_mean  t_min  t_max       p_mean   p_min   p_max  \\\n",
            "day_date                                                          \n",
            "2020-01-01  6.129167    4.9    8.7  1014.779167  1011.5  1020.7   \n",
            "2020-01-02  3.983333    2.9    4.7  1025.512500  1021.2  1028.2   \n",
            "2020-01-03  5.200000    3.1    8.0  1023.545833  1019.9  1027.2   \n",
            "2020-01-04  7.933333    6.9    8.6  1016.208333  1013.7  1019.4   \n",
            "2020-01-05  8.400000    7.3    9.0  1013.220833  1012.0  1015.6   \n",
            "\n",
            "            humidity_mean  humidity_min  precip_total_day  cloudcover_mean  \\\n",
            "day_date                                                                     \n",
            "2020-01-01      80.458333            72               6.7        81.958333   \n",
            "2020-01-02      79.041667            64               0.7        58.125000   \n",
            "2020-01-03      72.708333            63               0.1        69.291667   \n",
            "2020-01-04      71.291667            61               1.5        54.083333   \n",
            "2020-01-05      78.666667            66               4.3        86.791667   \n",
            "\n",
            "            weather_code_mode  \n",
            "day_date                       \n",
            "2020-01-01                 51  \n",
            "2020-01-02                  1  \n",
            "2020-01-03                  3  \n",
            "2020-01-04                  1  \n",
            "2020-01-05                 51  \n",
            "\n",
            "Размер агрегированных данных: (365, 11)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 365 entries, 2020-01-01 to 2020-12-31\n",
            "Data columns (total 11 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   t_mean             365 non-null    float64\n",
            " 1   t_min              365 non-null    float64\n",
            " 2   t_max              365 non-null    float64\n",
            " 3   p_mean             365 non-null    float64\n",
            " 4   p_min              365 non-null    float64\n",
            " 5   p_max              365 non-null    float64\n",
            " 6   humidity_mean      365 non-null    float64\n",
            " 7   humidity_min       365 non-null    int64  \n",
            " 8   precip_total_day   365 non-null    float64\n",
            " 9   cloudcover_mean    365 non-null    float64\n",
            " 10  weather_code_mode  365 non-null    int64  \n",
            "dtypes: float64(9), int64(2)\n",
            "memory usage: 34.2 KB\n",
            "\n",
            "--- Шаг 3 ЗАВЕРШЕН ---\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "print(\"\\n--- Шаг 4: Создание Мастер-сетки (Склад-Штабель-День) ---\")\n",
        "\n",
        "# Проверяем наличие необходимых DataFrame'ов\n",
        "if ('df_supplies_a1' not in locals() or df_supplies_a1.empty) and \\\n",
        "   ('df_temp_a1' not in locals() or df_temp_a1.empty):\n",
        "    print(\"Ошибка: Не найдены DataFrame'ы df_supplies_a1 и df_temp_a1 для определения уникальных штабелей.\")\n",
        "    master_grid = pd.DataFrame() # Создаем пустой DataFrame\n",
        "else:\n",
        "    # 4.1 Определяем временной диапазон (Январь 2019 - Декабрь 2020)\n",
        "    start_date = pd.Timestamp('2019-01-01')\n",
        "    end_date = pd.Timestamp('2020-12-31')\n",
        "    # Создаем полный диапазон дат с шагом в 1 день\n",
        "    all_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "    print(f\"Временной диапазон: с {start_date.date()} по {end_date.date()}\")\n",
        "    print(f\"Всего дней в диапазоне: {len(all_dates)}\")\n",
        "\n",
        "    # 4.2 Находим уникальные комбинации Склад/Штабель\n",
        "    unique_stacks = set()\n",
        "\n",
        "    # Из df_supplies_a1\n",
        "    if 'df_supplies_a1' in locals() and not df_supplies_a1.empty:\n",
        "        # Убедимся, что типы совместимы (например, оба строки или оба числа)\n",
        "        # Преобразуем в строки для надежности сравнения и объединения\n",
        "        stacks_supplies = df_supplies_a1[['Склад', 'Штабель']].astype(str).drop_duplicates()\n",
        "        unique_stacks.update(list(map(tuple, stacks_supplies.values))) # Добавляем как кортежи\n",
        "        print(f\" Уникальных пар (Склад, Штабель) найдено в df_supplies_a1: {len(stacks_supplies)}\")\n",
        "\n",
        "    # Из df_temp_a1\n",
        "    if 'df_temp_a1' in locals() and not df_temp_a1.empty:\n",
        "        stacks_temp = df_temp_a1[['Склад', 'Штабель']].astype(str).drop_duplicates()\n",
        "        unique_stacks.update(list(map(tuple, stacks_temp.values))) # Добавляем как кортежи\n",
        "        print(f\" Уникальных пар (Склад, Штабель) найдено в df_temp_a1: {len(stacks_temp)}\")\n",
        "\n",
        "    # Из df_fires_a1 (на всякий случай, если какой-то горевший штабель не попал в другие таблицы)\n",
        "    if 'df_fires_a1' in locals() and not df_fires_a1.empty:\n",
        "        stacks_fires = df_fires_a1[['Склад', 'Штабель']].astype(str).drop_duplicates()\n",
        "        unique_stacks.update(list(map(tuple, stacks_fires.values)))\n",
        "        print(f\" Уникальных пар (Склад, Штабель) найдено в df_fires_a1: {len(stacks_fires)}\")\n",
        "\n",
        "\n",
        "    # Преобразуем set обратно в DataFrame\n",
        "    if unique_stacks:\n",
        "        unique_stacks_df = pd.DataFrame(list(unique_stacks), columns=['Склад', 'Штабель'])\n",
        "        # Опционально: можно преобразовать обратно в числовые типы, если нужно\n",
        "        # unique_stacks_df['Склад'] = pd.to_numeric(unique_stacks_df['Склад'], errors='ignore')\n",
        "        # unique_stacks_df['Штабель'] = pd.to_numeric(unique_stacks_df['Штабель'], errors='ignore')\n",
        "\n",
        "        print(f\"\\nОбщее количество уникальных пар (Склад, Штабель): {len(unique_stacks_df)}\")\n",
        "        # print(\"Пример уникальных пар:\")\n",
        "        # print(unique_stacks_df.head())\n",
        "\n",
        "        # 4.3 Создаем полную сетку (декартово произведение дат и штабелей)\n",
        "        print(\"\\nСоздание мастер-сетки (декартово произведение)...\")\n",
        "        # Создаем DataFrame с датами\n",
        "        dates_df = pd.DataFrame({'Дата': all_dates})\n",
        "\n",
        "        # Используем cross join (декартово произведение)\n",
        "        master_grid = unique_stacks_df.merge(dates_df, how='cross')\n",
        "\n",
        "        print(f\"Размер созданной мастер-сетки: {master_grid.shape}\")\n",
        "        print(\"(Ожидаемое количество строк: {} уникальных штабелей * {} дней = {})\".format(\n",
        "            len(unique_stacks_df), len(all_dates), len(unique_stacks_df) * len(all_dates)\n",
        "        ))\n",
        "        print(\"\\nПример мастер-сетки (master_grid):\")\n",
        "        print(master_grid.head())\n",
        "        master_grid.info()\n",
        "\n",
        "    else:\n",
        "        print(\"Ошибка: Не удалось найти ни одной уникальной пары (Склад, Штабель). Мастер-сетка не создана.\")\n",
        "        master_grid = pd.DataFrame()\n",
        "\n",
        "\n",
        "print(\"\\n--- Шаг 4 ЗАВЕРШЕН ---\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41AUeBDlwckE",
        "outputId": "e5a6d6d7-35c3-4b66-e7b9-6e7e222cb40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Шаг 4: Создание Мастер-сетки (Склад-Штабель-День) ---\n",
            "Временной диапазон: с 2019-01-01 по 2020-12-31\n",
            "Всего дней в диапазоне: 731\n",
            " Уникальных пар (Склад, Штабель) найдено в df_supplies_a1: 93\n",
            " Уникальных пар (Склад, Штабель) найдено в df_temp_a1: 69\n",
            "\n",
            "Общее количество уникальных пар (Склад, Штабель): 96\n",
            "\n",
            "Создание мастер-сетки (декартово произведение)...\n",
            "Размер созданной мастер-сетки: (70176, 3)\n",
            "(Ожидаемое количество строк: 96 уникальных штабелей * 731 дней = 70176)\n",
            "\n",
            "Пример мастер-сетки (master_grid):\n",
            "  Склад Штабель       Дата\n",
            "0     3       7 2019-01-01\n",
            "1     3       7 2019-01-02\n",
            "2     3       7 2019-01-03\n",
            "3     3       7 2019-01-04\n",
            "4     3       7 2019-01-05\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 70176 entries, 0 to 70175\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype         \n",
            "---  ------   --------------  -----         \n",
            " 0   Склад    70176 non-null  object        \n",
            " 1   Штабель  70176 non-null  object        \n",
            " 2   Дата     70176 non-null  datetime64[ns]\n",
            "dtypes: datetime64[ns](1), object(2)\n",
            "memory usage: 1.6+ MB\n",
            "\n",
            "--- Шаг 4 ЗАВЕРШЕН ---\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Шаг 4.Б: Сохранение Мастер-сетки ---\n",
        "print(\"\\n--- Шаг 4.Б: Сохранение Мастер-сетки ---\")\n",
        "\n",
        "save_master_grid_file = True # Поставь False, если не хочешь сохранять\n",
        "\n",
        "if save_master_grid_file:\n",
        "    if 'master_grid' in locals() and not master_grid.empty:\n",
        "        output_dir_grid = \"processed_data_final\" # Папка для промежуточных/финальных данных\n",
        "        if not os.path.exists(output_dir_grid):\n",
        "            os.makedirs(output_dir_grid)\n",
        "            print(f\"Создана директория: {output_dir_grid}\")\n",
        "\n",
        "        output_path = os.path.join(output_dir_grid, 'master_grid_sklad_shtabel_den.csv')\n",
        "        try:\n",
        "            master_grid.to_csv(output_path, index=False)\n",
        "            print(f\"Мастер-сетка успешно сохранена в файл: {output_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при сохранении мастер-сетки: {e}\")\n",
        "    else:\n",
        "        print(\"Мастер-сетка (master_grid) не найдена или пуста. Файл не сохранен.\")\n",
        "else:\n",
        "    print(\"Сохранение мастер-сетки пропущено (save_master_grid_file = False).\")\n",
        "\n",
        "print(\"\\n--- Шаг 4.Б ЗАВЕРШЕН ---\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUXDABM3w6SJ",
        "outputId": "8e680662-7aa2-4d27-817b-eef8ef49ed0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Шаг 4.Б: Сохранение Мастер-сетки ---\n",
            "Создана директория: processed_data_final\n",
            "Мастер-сетка успешно сохранена в файл: processed_data_final/master_grid_sklad_shtabel_den.csv\n",
            "\n",
            "--- Шаг 4.Б ЗАВЕРШЕН ---\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Шаг 5: Присоединение агрегированных данных о погоде к Мастер-сетке ---\")\n",
        "\n",
        "# Проверяем наличие необходимых DataFrame'ов\n",
        "if 'master_grid' in locals() and not master_grid.empty and \\\n",
        "   'df_weather_daily' in locals() and not df_weather_daily.empty:\n",
        "\n",
        "    # Убедимся, что 'Дата' в master_grid это datetime\n",
        "    if not pd.api.types.is_datetime64_any_dtype(master_grid['Дата']):\n",
        "        print(\" Предупреждение: Преобразование 'Дата' в master_grid в datetime...\")\n",
        "        master_grid['Дата'] = pd.to_datetime(master_grid['Дата'], errors='coerce')\n",
        "\n",
        "    # Убедимся, что индекс в df_weather_daily это datetime\n",
        "    if not isinstance(df_weather_daily.index, pd.DatetimeIndex):\n",
        "         print(\" Предупреждение: Преобразование индекса df_weather_daily в datetime...\")\n",
        "         try:\n",
        "             df_weather_daily.index = pd.to_datetime(df_weather_daily.index, errors='coerce')\n",
        "             # Проверим на NaT после преобразования\n",
        "             if df_weather_daily.index.isnull().any():\n",
        "                 print(\" Ошибка: В индексе df_weather_daily появились NaT после преобразования. Слияние невозможно.\")\n",
        "                 # Можно добавить код для удаления строк с NaT или остановки\n",
        "             else:\n",
        "                  print(\" Индекс df_weather_daily успешно преобразован.\")\n",
        "         except Exception as e:\n",
        "             print(f\" Ошибка при преобразовании индекса df_weather_daily: {e}. Слияние невозможно.\")\n",
        "             # Устанавливаем флаг ошибки или выходим\n",
        "             can_merge = False\n",
        "    else:\n",
        "        can_merge = True # Индекс уже datetime\n",
        "\n",
        "    # Проверяем, что нет NaT в ключевых столбцах перед слиянием\n",
        "    if master_grid['Дата'].isnull().any():\n",
        "        print(\" Предупреждение: В столбце 'Дата' мастер-сетки есть пропуски (NaT). Строки с пропусками не получат данных о погоде.\")\n",
        "        # Решение: либо удалить эти строки, либо оставить как есть\n",
        "\n",
        "    if can_merge:\n",
        "        print(\"\\nВыполнение слияния (merge) master_grid с df_weather_daily...\")\n",
        "        # Используем левое слияние: сохраняем все строки из master_grid\n",
        "        # Сливаем по 'Дата' из master_grid и по индексу из df_weather_daily\n",
        "        master_grid_with_weather = pd.merge(\n",
        "            master_grid,\n",
        "            df_weather_daily,\n",
        "            left_on='Дата',    # Ключ в левом DataFrame (master_grid)\n",
        "            right_index=True, # Использовать индекс как ключ в правом DataFrame (df_weather_daily)\n",
        "            how='left'        # Тип слияния: сохранить все строки из master_grid\n",
        "        )\n",
        "\n",
        "        print(f\"\\nРазмер сетки до слияния: {master_grid.shape}\")\n",
        "        print(f\"Размер сетки после слияния с погодой: {master_grid_with_weather.shape}\")\n",
        "\n",
        "        # Проверяем, появились ли новые столбцы\n",
        "        added_cols = set(master_grid_with_weather.columns) - set(master_grid.columns)\n",
        "        print(f\"Добавленные столбцы погоды: {list(added_cols)}\")\n",
        "\n",
        "        # Заменяем старую master_grid на обновленную\n",
        "        master_grid = master_grid_with_weather\n",
        "        print(\"\\nМастер-сетка обновлена данными о погоде.\")\n",
        "\n",
        "        print(\"\\nПример обновленной мастер-сетки (первые строки):\")\n",
        "        print(master_grid.head())\n",
        "        print(\"\\nИнформация об обновленной мастер-сетке:\")\n",
        "        master_grid.info()\n",
        "\n",
        "        # Проверим процент пропусков в добавленных колонках (ожидаемо, если погода не за все дни)\n",
        "        print(\"\\nПроцент пропусков в добавленных колонках погоды:\")\n",
        "        missing_weather_pct = master_grid[list(added_cols)].isnull().mean() * 100\n",
        "        print(missing_weather_pct.round(2))\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Ошибка: Один или оба DataFrame'а (master_grid, df_weather_daily) не найдены или пусты. Слияние невозможно.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Шаг 5 ЗАВЕРШЕН ---\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F551NCwGyHC_",
        "outputId": "83cab462-f972-4372-8e0c-92d1331a0819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Шаг 5: Присоединение агрегированных данных о погоде к Мастер-сетке ---\n",
            "\n",
            "Выполнение слияния (merge) master_grid с df_weather_daily...\n",
            "\n",
            "Размер сетки до слияния: (70176, 3)\n",
            "Размер сетки после слияния с погодой: (70176, 14)\n",
            "Добавленные столбцы погоды: ['p_min', 'weather_code_mode', 't_mean', 't_min', 'p_max', 'p_mean', 'humidity_min', 'precip_total_day', 't_max', 'cloudcover_mean', 'humidity_mean']\n",
            "\n",
            "Мастер-сетка обновлена данными о погоде.\n",
            "\n",
            "Пример обновленной мастер-сетки (первые строки):\n",
            "  Склад Штабель       Дата  t_mean  t_min  t_max  p_mean  p_min  p_max  \\\n",
            "0     3       7 2019-01-01     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "1     3       7 2019-01-02     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "2     3       7 2019-01-03     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "3     3       7 2019-01-04     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "4     3       7 2019-01-05     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "\n",
            "   humidity_mean  humidity_min  precip_total_day  cloudcover_mean  \\\n",
            "0            NaN           NaN               NaN              NaN   \n",
            "1            NaN           NaN               NaN              NaN   \n",
            "2            NaN           NaN               NaN              NaN   \n",
            "3            NaN           NaN               NaN              NaN   \n",
            "4            NaN           NaN               NaN              NaN   \n",
            "\n",
            "   weather_code_mode  \n",
            "0                NaN  \n",
            "1                NaN  \n",
            "2                NaN  \n",
            "3                NaN  \n",
            "4                NaN  \n",
            "\n",
            "Информация об обновленной мастер-сетке:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 70176 entries, 0 to 70175\n",
            "Data columns (total 14 columns):\n",
            " #   Column             Non-Null Count  Dtype         \n",
            "---  ------             --------------  -----         \n",
            " 0   Склад              70176 non-null  object        \n",
            " 1   Штабель            70176 non-null  object        \n",
            " 2   Дата               70176 non-null  datetime64[ns]\n",
            " 3   t_mean             35040 non-null  float64       \n",
            " 4   t_min              35040 non-null  float64       \n",
            " 5   t_max              35040 non-null  float64       \n",
            " 6   p_mean             35040 non-null  float64       \n",
            " 7   p_min              35040 non-null  float64       \n",
            " 8   p_max              35040 non-null  float64       \n",
            " 9   humidity_mean      35040 non-null  float64       \n",
            " 10  humidity_min       35040 non-null  float64       \n",
            " 11  precip_total_day   35040 non-null  float64       \n",
            " 12  cloudcover_mean    35040 non-null  float64       \n",
            " 13  weather_code_mode  35040 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(11), object(2)\n",
            "memory usage: 7.5+ MB\n",
            "\n",
            "Процент пропусков в добавленных колонках погоды:\n",
            "p_min                50.07\n",
            "weather_code_mode    50.07\n",
            "t_mean               50.07\n",
            "t_min                50.07\n",
            "p_max                50.07\n",
            "p_mean               50.07\n",
            "humidity_min         50.07\n",
            "precip_total_day     50.07\n",
            "t_max                50.07\n",
            "cloudcover_mean      50.07\n",
            "humidity_mean        50.07\n",
            "dtype: float64\n",
            "\n",
            "--- Шаг 5 ЗАВЕРШЕН ---\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Шаг 6: Присоединение данных о температуре штабеля ---\")\n",
        "\n",
        "# Проверяем наличие необходимых DataFrame'ов\n",
        "if 'master_grid' in locals() and not master_grid.empty and \\\n",
        "   'df_temp_a1' in locals() and not df_temp_a1.empty:\n",
        "\n",
        "    # 6.1 Подготовка df_temp_a1 для слияния\n",
        "    print(\"Подготовка данных о температуре (df_temp_a1)...\")\n",
        "    temp_to_merge = df_temp_a1[['Склад', 'Штабель', 'Дата акта', 'Максимальная температура']].copy()\n",
        "\n",
        "    # Убедимся, что 'Дата акта' это datetime\n",
        "    if not pd.api.types.is_datetime64_any_dtype(temp_to_merge['Дата акта']):\n",
        "        print(\" Предупреждение: Преобразование 'Дата акта' в datetime...\")\n",
        "        temp_to_merge['Дата акта'] = pd.to_datetime(temp_to_merge['Дата акта'], errors='coerce')\n",
        "\n",
        "    # Извлекаем только дату (без времени) для слияния с дневной сеткой\n",
        "    # Используем dt.normalize() для обнуления времени, сохраняя тип datetime64[ns]\n",
        "    temp_to_merge['Дата'] = temp_to_merge['Дата акта'].dt.normalize()\n",
        "\n",
        "    # Преобразуем Склад/Штабель в строки для консистентности ключей слияния\n",
        "    temp_to_merge['Склад'] = temp_to_merge['Склад'].astype(str)\n",
        "    temp_to_merge['Штабель'] = temp_to_merge['Штабель'].astype(str)\n",
        "\n",
        "    # Обработка дубликатов: Если за один день было несколько замеров для одного штабеля,\n",
        "    # берем МАКСИМАЛЬНУЮ температуру за этот день.\n",
        "    print(\" Агрегация температуры: взятие max() для дубликатов Склад/Штабель/Дата...\")\n",
        "    temp_aggregated = temp_to_merge.groupby(['Склад', 'Штабель', 'Дата'], as_index=False)['Максимальная температура'].max()\n",
        "    print(f\" Размер данных температуры после агрегации: {temp_aggregated.shape}\")\n",
        "\n",
        "    # 6.2 Подготовка master_grid (проверка типов ключей)\n",
        "    if not pd.api.types.is_string_dtype(master_grid['Склад']):\n",
        "         print(\" Предупреждение: Преобразование 'Склад' в master_grid в строку...\")\n",
        "         master_grid['Склад'] = master_grid['Склад'].astype(str)\n",
        "    if not pd.api.types.is_string_dtype(master_grid['Штабель']):\n",
        "         print(\" Предупреждение: Преобразование 'Штабель' в master_grid в строку...\")\n",
        "         master_grid['Штабель'] = master_grid['Штабель'].astype(str)\n",
        "    if not pd.api.types.is_datetime64_any_dtype(master_grid['Дата']):\n",
        "         print(\" Предупреждение: Преобразование 'Дата' в master_grid в datetime...\")\n",
        "         master_grid['Дата'] = pd.to_datetime(master_grid['Дата'], errors='coerce')\n",
        "\n",
        "    # Удаляем строки с NaT в датах перед слиянием, если они есть\n",
        "    initial_rows = len(master_grid)\n",
        "    master_grid.dropna(subset=['Дата'], inplace=True)\n",
        "    if len(master_grid) < initial_rows:\n",
        "        print(f\" Предупреждение: Удалено {initial_rows - len(master_grid)} строк с NaT в 'Дата' из master_grid.\")\n",
        "\n",
        "    initial_rows_temp = len(temp_aggregated)\n",
        "    temp_aggregated.dropna(subset=['Дата', 'Склад', 'Штабель', 'Максимальная температура'], inplace=True)\n",
        "    if len(temp_aggregated) < initial_rows_temp:\n",
        "         print(f\" Предупреждение: Удалено {initial_rows_temp - len(temp_aggregated)} строк с NaN/NaT из temp_aggregated.\")\n",
        "\n",
        "\n",
        "    # 6.3 Выполнение слияния\n",
        "    print(\"\\nВыполнение слияния (merge) master_grid с temp_aggregated...\")\n",
        "    master_grid_with_temp = pd.merge(\n",
        "        master_grid,\n",
        "        temp_aggregated[['Склад', 'Штабель', 'Дата', 'Максимальная температура']],\n",
        "        on=['Склад', 'Штабель', 'Дата'], # Ключи слияния\n",
        "        how='left'                      # Сохраняем все строки из master_grid\n",
        "    )\n",
        "\n",
        "    print(f\"\\nРазмер сетки до слияния: {master_grid.shape}\")\n",
        "    print(f\"Размер сетки после слияния с температурой: {master_grid_with_temp.shape}\")\n",
        "\n",
        "    # Переименуем добавленную колонку для ясности\n",
        "    master_grid_with_temp.rename(columns={'Максимальная температура': 'Temp_Measure_Max'}, inplace=True)\n",
        "    print(\"Добавлен столбец 'Temp_Measure_Max'\")\n",
        "\n",
        "    # Заменяем старую master_grid на обновленную\n",
        "    master_grid = master_grid_with_temp\n",
        "    print(\"\\nМастер-сетка обновлена данными о температуре.\")\n",
        "\n",
        "    print(\"\\nПример обновленной мастер-сетки (первые строки):\")\n",
        "    print(master_grid.head())\n",
        "    # print(\"\\nИнформация об обновленной мастер-сетке:\")\n",
        "    # master_grid.info() # Может быть очень длинным, выведем только пропуски\n",
        "\n",
        "    # Проверим процент пропусков в добавленной колонке температуры\n",
        "    # Ожидается высокий процент, так как замеры не ежедневные\n",
        "    print(\"\\nПроцент пропусков в добавленном столбце 'Temp_Measure_Max':\")\n",
        "    missing_temp_pct = master_grid['Temp_Measure_Max'].isnull().mean() * 100\n",
        "    print(f\"{missing_temp_pct:.2f}%\")\n",
        "\n",
        "else:\n",
        "    print(\"Ошибка: Один или оба DataFrame'а (master_grid, df_temp_a1) не найдены или пусты. Слияние невозможно.\")\n",
        "\n",
        "print(\"\\n--- Шаг 6 ЗАВЕРШЕН ---\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGAvVQKryqTh",
        "outputId": "ccb29287-9dbd-40fe-fa18-cf3a4a143398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Шаг 6: Присоединение данных о температуре штабеля ---\n",
            "Подготовка данных о температуре (df_temp_a1)...\n",
            " Агрегация температуры: взятие max() для дубликатов Склад/Штабель/Дата...\n",
            " Размер данных температуры после агрегации: (2175, 4)\n",
            "\n",
            "Выполнение слияния (merge) master_grid с temp_aggregated...\n",
            "\n",
            "Размер сетки до слияния: (70176, 14)\n",
            "Размер сетки после слияния с температурой: (70176, 15)\n",
            "Добавлен столбец 'Temp_Measure_Max'\n",
            "\n",
            "Мастер-сетка обновлена данными о температуре.\n",
            "\n",
            "Пример обновленной мастер-сетки (первые строки):\n",
            "  Склад Штабель       Дата  t_mean  t_min  t_max  p_mean  p_min  p_max  \\\n",
            "0     3       7 2019-01-01     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "1     3       7 2019-01-02     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "2     3       7 2019-01-03     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "3     3       7 2019-01-04     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "4     3       7 2019-01-05     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "\n",
            "   humidity_mean  humidity_min  precip_total_day  cloudcover_mean  \\\n",
            "0            NaN           NaN               NaN              NaN   \n",
            "1            NaN           NaN               NaN              NaN   \n",
            "2            NaN           NaN               NaN              NaN   \n",
            "3            NaN           NaN               NaN              NaN   \n",
            "4            NaN           NaN               NaN              NaN   \n",
            "\n",
            "   weather_code_mode  Temp_Measure_Max  \n",
            "0                NaN               NaN  \n",
            "1                NaN               NaN  \n",
            "2                NaN               NaN  \n",
            "3                NaN               NaN  \n",
            "4                NaN               NaN  \n",
            "\n",
            "Процент пропусков в добавленном столбце 'Temp_Measure_Max':\n",
            "96.90%\n",
            "\n",
            "--- Шаг 6 ЗАВЕРШЕН ---\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n--- Шаг 7: Присоединение данных о поступлении/отгрузке угля (ОТЛАДОЧНАЯ ВЕРСИЯ) ---\")\n",
        "\n",
        "# Проверяем наличие необходимых DataFrame'ов\n",
        "if 'master_grid' not in locals() or master_grid.empty or \\\n",
        "   'df_supplies_a1' not in locals() or df_supplies_a1.empty:\n",
        "      print(\"Ошибка: Отсутствует master_grid или df_supplies_a1. Шаг 7 не может быть выполнен.\")\n",
        "else:\n",
        "    # 7.1 Подготовка df_supplies_a1\n",
        "    print(\"Подготовка данных о поставках (df_supplies_a1)...\")\n",
        "    supplies_to_agg = df_supplies_a1[['Склад', 'Штабель', 'ВыгрузкаНаСклад', 'ПогрузкаНаСудно', 'На склад, тн', 'На судно, тн']].copy()\n",
        "    print(f\" Исходный размер supplies_to_agg: {supplies_to_agg.shape}\")\n",
        "\n",
        "    # Преобразования типов (добавим проверки)\n",
        "    try:\n",
        "        if not pd.api.types.is_datetime64_any_dtype(supplies_to_agg['ВыгрузкаНаСклад']):\n",
        "            supplies_to_agg['ВыгрузкаНаСклад'] = pd.to_datetime(supplies_to_agg['ВыгрузкаНаСклад'], errors='coerce')\n",
        "        if not pd.api.types.is_datetime64_any_dtype(supplies_to_agg['ПогрузкаНаСудно']):\n",
        "            supplies_to_agg['ПогрузкаНаСудно'] = pd.to_datetime(supplies_to_agg['ПогрузкаНаСудно'], errors='coerce')\n",
        "        if not pd.api.types.is_numeric_dtype(supplies_to_agg['На склад, тн']):\n",
        "            supplies_to_agg['На склад, тн'] = pd.to_numeric(supplies_to_agg['На склад, тн'], errors='coerce')\n",
        "        if not pd.api.types.is_numeric_dtype(supplies_to_agg['На судно, тн']):\n",
        "            supplies_to_agg['На судно, тн'] = pd.to_numeric(supplies_to_agg['На судно, тн'], errors='coerce')\n",
        "        print(\" Типы данных преобразованы (или уже были корректны).\")\n",
        "    except Exception as e:\n",
        "        print(f\" ОШИБКА при преобразовании типов в supplies_to_agg: {e}\")\n",
        "\n",
        "    # Заполнение NaN нулями\n",
        "    supplies_to_agg['На склад, тн'] = supplies_to_agg['На склад, тн'].fillna(0)\n",
        "    supplies_to_agg['На судно, тн'] = supplies_to_agg['На судно, тн'].fillna(0)\n",
        "    print(\" NaN в тоннаже заменены на 0.\")\n",
        "\n",
        "    # --- Агрегация поступлений ---\n",
        "    print(\"\\nАгрегация поступлений...\")\n",
        "    arrivals = supplies_to_agg[supplies_to_agg['На склад, тн'] > 0].copy()\n",
        "    print(f\" Найдено строк с поступлениями (arrivals): {len(arrivals)}\")\n",
        "    if not arrivals.empty:\n",
        "        arrivals['Дата'] = arrivals['ВыгрузкаНаСклад'].dt.normalize()\n",
        "        # --- ОТЛАДКА ---\n",
        "        print(f\" Пример дат в arrivals['Дата']: {arrivals['Дата'].head().tolist()}\")\n",
        "        print(f\" Пропуски в arrivals['Дата']: {arrivals['Дата'].isnull().sum()}\")\n",
        "        arrivals.dropna(subset=['Дата'], inplace=True) # Удалим строки с некорректной датой выгрузки\n",
        "        print(f\" Строк arrivals после удаления NaN в дате: {len(arrivals)}\")\n",
        "        # --- КОНЕЦ ОТЛАДКИ ---\n",
        "        arrivals_agg = arrivals.groupby(['Склад', 'Штабель', 'Дата'], as_index=False)['На склад, тн'].sum()\n",
        "        arrivals_agg.rename(columns={'На склад, тн': 'Added_Today_Tons'}, inplace=True)\n",
        "        arrivals_agg['Склад'] = arrivals_agg['Склад'].astype(str)\n",
        "        arrivals_agg['Штабель'] = arrivals_agg['Штабель'].astype(str)\n",
        "        print(f\" Размер arrivals_agg (поступления по дням): {arrivals_agg.shape}\")\n",
        "        # --- ОТЛАДКА ---\n",
        "        print(\" Пример arrivals_agg:\")\n",
        "        print(arrivals_agg.head())\n",
        "        # --- КОНЕЦ ОТЛАДКИ ---\n",
        "    else:\n",
        "        arrivals_agg = pd.DataFrame(columns=['Склад', 'Штабель', 'Дата', 'Added_Today_Tons'])\n",
        "\n",
        "    # --- Агрегация отгрузок ---\n",
        "    print(\"\\nАгрегация отгрузок...\")\n",
        "    shipments = supplies_to_agg[supplies_to_agg['На судно, тн'] > 0].copy()\n",
        "    print(f\" Найдено строк с отгрузками (shipments): {len(shipments)}\")\n",
        "    if not shipments.empty:\n",
        "        shipments['Дата'] = shipments['ПогрузкаНаСудно'].dt.normalize()\n",
        "         # --- ОТЛАДКА ---\n",
        "        print(f\" Пример дат в shipments['Дата']: {shipments['Дата'].head().tolist()}\")\n",
        "        print(f\" Пропуски в shipments['Дата']: {shipments['Дата'].isnull().sum()}\")\n",
        "        shipments.dropna(subset=['Дата'], inplace=True) # Удалим строки с некорректной датой погрузки\n",
        "        print(f\" Строк shipments после удаления NaN в дате: {len(shipments)}\")\n",
        "        # --- КОНЕЦ ОТЛАДКИ ---\n",
        "        shipments_agg = shipments.groupby(['Склад', 'Штабель', 'Дата'], as_index=False)['На судно, тн'].sum()\n",
        "        shipments_agg.rename(columns={'На судно, тн': 'Removed_Today_Tons'}, inplace=True)\n",
        "        shipments_agg['Склад'] = shipments_agg['Склад'].astype(str)\n",
        "        shipments_agg['Штабель'] = shipments_agg['Штабель'].astype(str)\n",
        "        print(f\" Размер shipments_agg (отгрузки по дням): {shipments_agg.shape}\")\n",
        "         # --- ОТЛАДКА ---\n",
        "        print(\" Пример shipments_agg:\")\n",
        "        print(shipments_agg.head())\n",
        "        # --- КОНЕЦ ОТЛАДКИ ---\n",
        "    else:\n",
        "        shipments_agg = pd.DataFrame(columns=['Склад', 'Штабель', 'Дата', 'Removed_Today_Tons'])\n",
        "\n",
        "    # 7.2 Подготовка master_grid\n",
        "    print(\"\\nПодготовка master_grid к слиянию...\")\n",
        "    if not pd.api.types.is_string_dtype(master_grid['Склад']):\n",
        "         master_grid['Склад'] = master_grid['Склад'].astype(str)\n",
        "    if not pd.api.types.is_string_dtype(master_grid['Штабель']):\n",
        "         master_grid['Штабель'] = master_grid['Штабель'].astype(str)\n",
        "    if not pd.api.types.is_datetime64_any_dtype(master_grid['Дата']):\n",
        "         master_grid['Дата'] = pd.to_datetime(master_grid['Дата'], errors='coerce')\n",
        "    initial_rows = len(master_grid)\n",
        "    master_grid.dropna(subset=['Дата'], inplace=True)\n",
        "    if len(master_grid) < initial_rows:\n",
        "         print(f\" Удалено {initial_rows - len(master_grid)} строк с NaT в 'Дата' из master_grid.\")\n",
        "    print(\" Типы ключей в master_grid проверены/преобразованы.\")\n",
        "     # --- ОТЛАДКА ---\n",
        "    print(f\" Типы ключей в master_grid: Склад={master_grid['Склад'].dtype}, Штабель={master_grid['Штабель'].dtype}, Дата={master_grid['Дата'].dtype}\")\n",
        "    print(f\" Типы ключей в arrivals_agg: Склад={arrivals_agg['Склад'].dtype if not arrivals_agg.empty else 'N/A'}, Штабель={arrivals_agg['Штабель'].dtype if not arrivals_agg.empty else 'N/A'}, Дата={arrivals_agg['Дата'].dtype if not arrivals_agg.empty else 'N/A'}\")\n",
        "    print(f\" Типы ключей в shipments_agg: Склад={shipments_agg['Склад'].dtype if not shipments_agg.empty else 'N/A'}, Штабель={shipments_agg['Штабель'].dtype if not shipments_agg.empty else 'N/A'}, Дата={shipments_agg['Дата'].dtype if not shipments_agg.empty else 'N/A'}\")\n",
        "     # --- КОНЕЦ ОТЛАДКИ ---\n",
        "\n",
        "\n",
        "    # 7.3 Выполнение слияния\n",
        "    print(\"\\nВыполнение слияния master_grid с arrivals_agg...\")\n",
        "    master_grid = pd.merge(\n",
        "        master_grid,\n",
        "        arrivals_agg[['Склад', 'Штабель', 'Дата', 'Added_Today_Tons']],\n",
        "        on=['Склад', 'Штабель', 'Дата'],\n",
        "        how='left'\n",
        "    )\n",
        "     # --- ОТЛАДКА ---\n",
        "    print(f\" Колонки в master_grid ПОСЛЕ слияния с arrivals_agg: {master_grid.columns.tolist()}\")\n",
        "    print(f\" Проверка NaN в Added_Today_Tons СРАЗУ ПОСЛЕ слияния: {master_grid['Added_Today_Tons'].isnull().sum() if 'Added_Today_Tons' in master_grid.columns else 'Колонка НЕ добавлена!'}\")\n",
        "     # --- КОНЕЦ ОТЛАДКИ ---\n",
        "\n",
        "    print(\"\\nВыполнение слияния master_grid с shipments_agg...\")\n",
        "    master_grid = pd.merge(\n",
        "        master_grid,\n",
        "        shipments_agg[['Склад', 'Штабель', 'Дата', 'Removed_Today_Tons']],\n",
        "        on=['Склад', 'Штабель', 'Дата'],\n",
        "        how='left'\n",
        "    )\n",
        "     # --- ОТЛАДКА ---\n",
        "    print(f\" Колонки в master_grid ПОСЛЕ слияния с shipments_agg: {master_grid.columns.tolist()}\")\n",
        "    print(f\" Проверка NaN в Removed_Today_Tons СРАЗУ ПОСЛЕ слияния: {master_grid['Removed_Today_Tons'].isnull().sum() if 'Removed_Today_Tons' in master_grid.columns else 'Колонка НЕ добавлена!'}\")\n",
        "     # --- КОНЕЦ ОТЛАДКИ ---\n",
        "\n",
        "    # 7.4 Заполнение NaN нулями\n",
        "    print(\"\\nЗаполнение NaN нулями в 'Added_Today_Tons' и 'Removed_Today_Tons'...\")\n",
        "    if 'Added_Today_Tons' in master_grid.columns:\n",
        "        master_grid['Added_Today_Tons'] = master_grid['Added_Today_Tons'].fillna(0)\n",
        "        print(\" NaN в Added_Today_Tons заменены на 0.\")\n",
        "    else:\n",
        "        print(\" ПРЕДУПРЕЖДЕНИЕ: Колонка Added_Today_Tons не найдена для заполнения NaN!\")\n",
        "\n",
        "    if 'Removed_Today_Tons' in master_grid.columns:\n",
        "        master_grid['Removed_Today_Tons'] = master_grid['Removed_Today_Tons'].fillna(0)\n",
        "        print(\" NaN в Removed_Today_Tons заменены на 0.\")\n",
        "    else:\n",
        "        print(\" ПРЕДУПРЕЖДЕНИЕ: Колонка Removed_Today_Tons не найдена для заполнения NaN!\")\n",
        "\n",
        "    print(\"\\nМастер-сетка обновлена данными о поступлениях/отгрузках.\")\n",
        "    print(\"\\nФинальный набор колонок в master_grid:\", master_grid.columns.tolist())\n",
        "    print(\"\\nПример финальной мастер-сетки (Шаг 7):\")\n",
        "    print(master_grid.head())\n",
        "\n",
        "print(\"\\n--- Шаг 7 (ОТЛАДОЧНАЯ ВЕРСИЯ) ЗАВЕРШЕН ---\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l04eC-L06l3o",
        "outputId": "b8790928-42d1-478b-d496-8cafceefa7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Шаг 7: Присоединение данных о поступлении/отгрузке угля (ОТЛАДОЧНАЯ ВЕРСИЯ) ---\n",
            "Подготовка данных о поставках (df_supplies_a1)...\n",
            " Исходный размер supplies_to_agg: (4285, 6)\n",
            " Типы данных преобразованы (или уже были корректны).\n",
            " NaN в тоннаже заменены на 0.\n",
            "\n",
            "Агрегация поступлений...\n",
            " Найдено строк с поступлениями (arrivals): 4285\n",
            " Пример дат в arrivals['Дата']: [Timestamp('2019-01-02 00:00:00'), Timestamp('2019-01-02 00:00:00'), Timestamp('2019-01-02 00:00:00'), Timestamp('2019-01-03 00:00:00'), Timestamp('2019-01-03 00:00:00')]\n",
            " Пропуски в arrivals['Дата']: 0\n",
            " Строк arrivals после удаления NaN в дате: 4285\n",
            " Размер arrivals_agg (поступления по дням): (3260, 4)\n",
            " Пример arrivals_agg:\n",
            "  Склад Штабель       Дата  Added_Today_Tons\n",
            "0     3       1 2019-07-18        41129.4515\n",
            "1     3       1 2019-07-19        17861.4665\n",
            "2     3       1 2019-07-20        24048.4975\n",
            "3     3       1 2019-07-21          749.8105\n",
            "4     3       1 2019-07-22          199.4695\n",
            "\n",
            "Агрегация отгрузок...\n",
            " Найдено строк с отгрузками (shipments): 4283\n",
            " Пример дат в shipments['Дата']: [Timestamp('2019-02-12 00:00:00'), Timestamp('2019-02-23 00:00:00'), Timestamp('2019-01-04 00:00:00'), Timestamp('2019-01-22 00:00:00'), Timestamp('2019-02-23 00:00:00')]\n",
            " Пропуски в shipments['Дата']: 0\n",
            " Строк shipments после удаления NaN в дате: 4283\n",
            " Размер shipments_agg (отгрузки по дням): (727, 4)\n",
            " Пример shipments_agg:\n",
            "  Склад Штабель       Дата  Removed_Today_Tons\n",
            "0     3       1 2019-07-31          44504.4700\n",
            "1     3       1 2019-08-09         107687.5795\n",
            "2     3       1 2019-09-10          45490.5380\n",
            "3     3       4 2019-04-12          81906.0170\n",
            "4     3       4 2019-05-02          66503.3455\n",
            "\n",
            "Подготовка master_grid к слиянию...\n",
            " Типы ключей в master_grid проверены/преобразованы.\n",
            " Типы ключей в master_grid: Склад=object, Штабель=object, Дата=datetime64[ns]\n",
            " Типы ключей в arrivals_agg: Склад=object, Штабель=object, Дата=datetime64[ns]\n",
            " Типы ключей в shipments_agg: Склад=object, Штабель=object, Дата=datetime64[ns]\n",
            "\n",
            "Выполнение слияния master_grid с arrivals_agg...\n",
            " Колонки в master_grid ПОСЛЕ слияния с arrivals_agg: ['Склад', 'Штабель', 'Дата', 't_mean', 't_min', 't_max', 'p_mean', 'p_min', 'p_max', 'humidity_mean', 'humidity_min', 'precip_total_day', 'cloudcover_mean', 'weather_code_mode', 'Temp_Measure_Max', 'Added_Today_Tons']\n",
            " Проверка NaN в Added_Today_Tons СРАЗУ ПОСЛЕ слияния: 66916\n",
            "\n",
            "Выполнение слияния master_grid с shipments_agg...\n",
            " Колонки в master_grid ПОСЛЕ слияния с shipments_agg: ['Склад', 'Штабель', 'Дата', 't_mean', 't_min', 't_max', 'p_mean', 'p_min', 'p_max', 'humidity_mean', 'humidity_min', 'precip_total_day', 'cloudcover_mean', 'weather_code_mode', 'Temp_Measure_Max', 'Added_Today_Tons', 'Removed_Today_Tons']\n",
            " Проверка NaN в Removed_Today_Tons СРАЗУ ПОСЛЕ слияния: 69449\n",
            "\n",
            "Заполнение NaN нулями в 'Added_Today_Tons' и 'Removed_Today_Tons'...\n",
            " NaN в Added_Today_Tons заменены на 0.\n",
            " NaN в Removed_Today_Tons заменены на 0.\n",
            "\n",
            "Мастер-сетка обновлена данными о поступлениях/отгрузках.\n",
            "\n",
            "Финальный набор колонок в master_grid: ['Склад', 'Штабель', 'Дата', 't_mean', 't_min', 't_max', 'p_mean', 'p_min', 'p_max', 'humidity_mean', 'humidity_min', 'precip_total_day', 'cloudcover_mean', 'weather_code_mode', 'Temp_Measure_Max', 'Added_Today_Tons', 'Removed_Today_Tons']\n",
            "\n",
            "Пример финальной мастер-сетки (Шаг 7):\n",
            "  Склад Штабель       Дата  t_mean  t_min  t_max  p_mean  p_min  p_max  \\\n",
            "0     3       7 2019-01-01     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "1     3       7 2019-01-02     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "2     3       7 2019-01-03     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "3     3       7 2019-01-04     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "4     3       7 2019-01-05     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "\n",
            "   humidity_mean  humidity_min  precip_total_day  cloudcover_mean  \\\n",
            "0            NaN           NaN               NaN              NaN   \n",
            "1            NaN           NaN               NaN              NaN   \n",
            "2            NaN           NaN               NaN              NaN   \n",
            "3            NaN           NaN               NaN              NaN   \n",
            "4            NaN           NaN               NaN              NaN   \n",
            "\n",
            "   weather_code_mode  Temp_Measure_Max  Added_Today_Tons  Removed_Today_Tons  \n",
            "0                NaN               NaN               0.0                 0.0  \n",
            "1                NaN               NaN               0.0                 0.0  \n",
            "2                NaN               NaN               0.0                 0.0  \n",
            "3                NaN               NaN               0.0                 0.0  \n",
            "4                NaN               NaN               0.0                 0.0  \n",
            "\n",
            "--- Шаг 7 (ОТЛАДОЧНАЯ ВЕРСИЯ) ЗАВЕРШЕН ---\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"\\n--- Шаг 8: Расчет динамических признаков (Вес, Возраст, Дни с замера) ---\")\n",
        "\n",
        "# Проверяем наличие master_grid\n",
        "if 'master_grid' not in locals() or master_grid.empty:\n",
        "    print(\"Ошибка: DataFrame 'master_grid' не найден или пуст. Расчет невозможен.\")\n",
        "else:\n",
        "    # --- 8.1 Расчет текущего веса ---\n",
        "    print(\"Расчет текущего оценочного веса штабеля ('Current_Weight_Tons')...\")\n",
        "\n",
        "    # Убедимся, что необходимые колонки существуют и числовые\n",
        "    if 'Added_Today_Tons' in master_grid.columns and \\\n",
        "       'Removed_Today_Tons' in master_grid.columns and \\\n",
        "       pd.api.types.is_numeric_dtype(master_grid['Added_Today_Tons']) and \\\n",
        "       pd.api.types.is_numeric_dtype(master_grid['Removed_Today_Tons']):\n",
        "\n",
        "        # Рассчитываем дневное изменение веса\n",
        "        master_grid['Daily_Weight_Change'] = master_grid['Added_Today_Tons'] - master_grid['Removed_Today_Tons']\n",
        "\n",
        "        # Сортируем данные - КРИТИЧЕСКИ ВАЖНО для кумулятивных расчетов!\n",
        "        print(\" Сортировка данных по Склад, Штабель, Дата...\")\n",
        "        master_grid.sort_values(by=['Склад', 'Штабель', 'Дата'], inplace=True)\n",
        "\n",
        "        # Рассчитываем кумулятивную сумму изменения веса в рамках каждого штабеля\n",
        "        print(\" Расчет кумулятивной суммы веса...\")\n",
        "        master_grid['Current_Weight_Tons'] = master_grid.groupby(['Склад', 'Штабель'])['Daily_Weight_Change'].cumsum()\n",
        "\n",
        "        # Проверка на отрицательный вес (может указывать на проблемы с данными)\n",
        "        neg_weight_count = (master_grid['Current_Weight_Tons'] < -0.01).sum() # Небольшой допуск на ошибки округления\n",
        "        if neg_weight_count > 0:\n",
        "            print(f\" Предупреждение: Обнаружено {neg_weight_count} строк с отрицательным расчетным весом (< -0.01).\")\n",
        "            # По желанию можно установить минимальный вес 0:\n",
        "            # master_grid['Current_Weight_Tons'] = master_grid['Current_Weight_Tons'].clip(lower=0)\n",
        "            # print(\" Отрицательный вес был заменен на 0.\")\n",
        "    else:\n",
        "        print(\" Ошибка: Колонки 'Added_Today_Tons' или 'Removed_Today_Tons' отсутствуют или не являются числовыми.\")\n",
        "        master_grid['Current_Weight_Tons'] = np.nan # Добавляем колонку с NaN\n",
        "\n",
        "    # --- 8.2 Расчет возраста штабеля ---\n",
        "    print(\"\\nРасчет возраста штабеля ('Stack_Age_Days')...\")\n",
        "    # Используем дату первой выгрузки на склад как дату формирования (см. обсуждение ограничений)\n",
        "    if 'df_supplies_a1' in locals() and not df_supplies_a1.empty:\n",
        "        print(\" Поиск даты первого поступления для каждого штабеля...\")\n",
        "        arrivals_data = df_supplies_a1[df_supplies_a1['На склад, тн'] > 0].copy()\n",
        "        if 'ВыгрузкаНаСклад' in arrivals_data.columns:\n",
        "             # Убедимся что дата - datetime\n",
        "             if not pd.api.types.is_datetime64_any_dtype(arrivals_data['ВыгрузкаНаСклад']):\n",
        "                  arrivals_data['ВыгрузкаНаСклад'] = pd.to_datetime(arrivals_data['ВыгрузкаНаСклад'], errors='coerce')\n",
        "\n",
        "             # Находим минимальную дату для каждой пары Склад/Штабель\n",
        "             formation_dates = arrivals_data.loc[arrivals_data['ВыгрузкаНаСклад'].notna()].groupby(['Склад', 'Штабель'])['ВыгрузкаНаСклад'].min().reset_index()\n",
        "             formation_dates.rename(columns={'ВыгрузкаНаСклад': 'Formation_Date'}, inplace=True)\n",
        "\n",
        "             # Приводим ключи к строкам для слияния\n",
        "             formation_dates['Склад'] = formation_dates['Склад'].astype(str)\n",
        "             formation_dates['Штабель'] = formation_dates['Штабель'].astype(str)\n",
        "             print(f\" Найдено дат формирования: {len(formation_dates)}\")\n",
        "\n",
        "             # Присоединяем дату формирования к master_grid\n",
        "             print(\" Присоединение даты формирования...\")\n",
        "             master_grid = pd.merge(\n",
        "                 master_grid,\n",
        "                 formation_dates,\n",
        "                 on=['Склад', 'Штабель'],\n",
        "                 how='left'\n",
        "             )\n",
        "\n",
        "             # Рассчитываем возраст в днях\n",
        "             if 'Formation_Date' in master_grid.columns:\n",
        "                 print(\" Расчет возраста в днях...\")\n",
        "                 # Убедимся, что обе колонки - datetime\n",
        "                 if not pd.api.types.is_datetime64_any_dtype(master_grid['Дата']):\n",
        "                     master_grid['Дата'] = pd.to_datetime(master_grid['Дата'], errors='coerce')\n",
        "                 if not pd.api.types.is_datetime64_any_dtype(master_grid['Formation_Date']):\n",
        "                      master_grid['Formation_Date'] = pd.to_datetime(master_grid['Formation_Date'], errors='coerce')\n",
        "\n",
        "                 # Расчет разницы\n",
        "                 time_diff = master_grid['Дата'] - master_grid['Formation_Date']\n",
        "                 # Берем только количество дней, обрабатываем случаи до даты формирования или NaT\n",
        "                 master_grid['Stack_Age_Days'] = time_diff.dt.days\n",
        "                 master_grid.loc[master_grid['Stack_Age_Days'] < 0, 'Stack_Age_Days'] = 0 # Возраст не может быть < 0\n",
        "                 # Пропуски в возрасте возникнут, если не было даты формирования\n",
        "\n",
        "                 # Удаляем вспомогательную колонку даты формирования (если не нужна)\n",
        "                 # master_grid.drop(columns=['Formation_Date'], inplace=True)\n",
        "\n",
        "             else:\n",
        "                 print(\" Ошибка: Не удалось присоединить 'Formation_Date'.\")\n",
        "                 master_grid['Stack_Age_Days'] = np.nan\n",
        "\n",
        "        else:\n",
        "             print(\" Ошибка: Отсутствует колонка 'ВыгрузкаНаСклад' в данных о поступлениях.\")\n",
        "             master_grid['Stack_Age_Days'] = np.nan\n",
        "    else:\n",
        "        print(\" Предупреждение: DataFrame df_supplies_a1 не найден. Возраст штабеля не рассчитан.\")\n",
        "        master_grid['Stack_Age_Days'] = np.nan\n",
        "\n",
        "    # --- 8.3 Расчет дней с последнего замера температуры ---\n",
        "    print(\"\\nРасчет дней с последнего замера температуры ('Days_Since_Last_Measurement')...\")\n",
        "    if 'Temp_Measure_Max' in master_grid.columns:\n",
        "        # Создаем колонку с датой только там, где был замер\n",
        "        master_grid['Measure_Date_If_Present'] = master_grid['Дата'].where(master_grid['Temp_Measure_Max'].notna())\n",
        "\n",
        "        # Сортировка (уже должна быть, но для надежности)\n",
        "        master_grid.sort_values(by=['Склад', 'Штабель', 'Дата'], inplace=True)\n",
        "\n",
        "        # Заполняем пропуски датой последнего замера (forward fill) внутри каждой группы\n",
        "        print(\" Применение forward fill для даты последнего замера...\")\n",
        "        master_grid['Last_Known_Measure_Date'] = master_grid.groupby(['Склад', 'Штабель'])['Measure_Date_If_Present'].ffill()\n",
        "\n",
        "        # Рассчитываем разницу в днях\n",
        "        if 'Last_Known_Measure_Date' in master_grid.columns:\n",
        "             # Убедимся, что Last_Known_Measure_Date - datetime\n",
        "             if not pd.api.types.is_datetime64_any_dtype(master_grid['Last_Known_Measure_Date']):\n",
        "                  master_grid['Last_Known_Measure_Date'] = pd.to_datetime(master_grid['Last_Known_Measure_Date'], errors='coerce')\n",
        "\n",
        "             time_diff_measure = master_grid['Дата'] - master_grid['Last_Known_Measure_Date']\n",
        "             master_grid['Days_Since_Last_Measurement'] = time_diff_measure.dt.days\n",
        "             # Пропуски в результате будут там, где еще не было ни одного замера для штабеля\n",
        "\n",
        "             # Удаляем вспомогательные колонки\n",
        "             master_grid.drop(columns=['Measure_Date_If_Present', 'Last_Known_Measure_Date'], inplace=True, errors='ignore')\n",
        "        else:\n",
        "             print(\" Ошибка: Не удалось создать 'Last_Known_Measure_Date'.\")\n",
        "             master_grid['Days_Since_Last_Measurement'] = np.nan\n",
        "\n",
        "    else:\n",
        "        print(\" Предупреждение: Столбец 'Temp_Measure_Max' отсутствует. Расчет дней с замера невозможен.\")\n",
        "        master_grid['Days_Since_Last_Measurement'] = np.nan\n",
        "\n",
        "\n",
        "    # --- 8.4 Вывод информации ---\n",
        "    print(\"\\nРасчет динамических признаков завершен.\")\n",
        "    print(\"Добавленные/обновленные колонки: 'Daily_Weight_Change', 'Current_Weight_Tons', 'Stack_Age_Days', 'Days_Since_Last_Measurement'\")\n",
        "\n",
        "    print(\"\\nПример обновленной мастер-сетки:\")\n",
        "    print(master_grid.head())\n",
        "\n",
        "    print(\"\\nПроверка пропусков в новых колонках:\")\n",
        "    new_cols = ['Current_Weight_Tons', 'Stack_Age_Days', 'Days_Since_Last_Measurement']\n",
        "    for col in new_cols:\n",
        "        if col in master_grid.columns:\n",
        "            missing_pct = master_grid[col].isnull().mean() * 100\n",
        "            print(f\" {col}: {missing_pct:.2f}% пропусков\")\n",
        "        else:\n",
        "            print(f\" {col}: столбец не найден\")\n",
        "\n",
        "\n",
        "    # --- 8.5 Сохранение результата ---\n",
        "    print(\"\\n--- Сохранение мастер-сетки с динамическими признаками ---\")\n",
        "    output_dir_final = \"processed_data_final\"\n",
        "    if not os.path.exists(output_dir_final):\n",
        "        os.makedirs(output_dir_final)\n",
        "        print(f\"Создана директория: {output_dir_final}\")\n",
        "\n",
        "    output_path_dynamic = os.path.join(output_dir_final, 'master_grid_with_dynamic_features.csv')\n",
        "    try:\n",
        "        master_grid.to_csv(output_path_dynamic, index=False)\n",
        "        print(f\"Мастер-сетка с динамическими признаками успешно сохранена в файл: {output_path_dynamic}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при сохранении файла: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Шаг 8 ЗАВЕРШЕН ---\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOgagu2X4pOP",
        "outputId": "4305c6a1-7dc1-4ffe-8643-104b27b282b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Шаг 8: Расчет динамических признаков (Вес, Возраст, Дни с замера) ---\n",
            "Расчет текущего оценочного веса штабеля ('Current_Weight_Tons')...\n",
            " Сортировка данных по Склад, Штабель, Дата...\n",
            " Расчет кумулятивной суммы веса...\n",
            " Предупреждение: Обнаружено 650 строк с отрицательным расчетным весом (< -0.01).\n",
            "\n",
            "Расчет возраста штабеля ('Stack_Age_Days')...\n",
            " Поиск даты первого поступления для каждого штабеля...\n",
            " Найдено дат формирования: 93\n",
            " Присоединение даты формирования...\n",
            " Расчет возраста в днях...\n",
            "\n",
            "Расчет дней с последнего замера температуры ('Days_Since_Last_Measurement')...\n",
            " Применение forward fill для даты последнего замера...\n",
            "\n",
            "Расчет динамических признаков завершен.\n",
            "Добавленные/обновленные колонки: 'Daily_Weight_Change', 'Current_Weight_Tons', 'Stack_Age_Days', 'Days_Since_Last_Measurement'\n",
            "\n",
            "Пример обновленной мастер-сетки:\n",
            "  Склад Штабель       Дата  t_mean  t_min  t_max  p_mean  p_min  p_max  \\\n",
            "0     3       0 2019-01-01     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "1     3       0 2019-01-02     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "2     3       0 2019-01-03     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "3     3       0 2019-01-04     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "4     3       0 2019-01-05     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "\n",
            "   humidity_mean  ...  cloudcover_mean  weather_code_mode  Temp_Measure_Max  \\\n",
            "0            NaN  ...              NaN                NaN               NaN   \n",
            "1            NaN  ...              NaN                NaN               NaN   \n",
            "2            NaN  ...              NaN                NaN               NaN   \n",
            "3            NaN  ...              NaN                NaN               NaN   \n",
            "4            NaN  ...              NaN                NaN               NaN   \n",
            "\n",
            "   Added_Today_Tons  Removed_Today_Tons  Daily_Weight_Change  \\\n",
            "0               0.0                 0.0                  0.0   \n",
            "1               0.0                 0.0                  0.0   \n",
            "2               0.0                 0.0                  0.0   \n",
            "3               0.0                 0.0                  0.0   \n",
            "4               0.0                 0.0                  0.0   \n",
            "\n",
            "   Current_Weight_Tons  Formation_Date  Stack_Age_Days  \\\n",
            "0                  0.0             NaT             NaN   \n",
            "1                  0.0             NaT             NaN   \n",
            "2                  0.0             NaT             NaN   \n",
            "3                  0.0             NaT             NaN   \n",
            "4                  0.0             NaT             NaN   \n",
            "\n",
            "  Days_Since_Last_Measurement  \n",
            "0                         NaN  \n",
            "1                         NaN  \n",
            "2                         NaN  \n",
            "3                         NaN  \n",
            "4                         NaN  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "\n",
            "Проверка пропусков в новых колонках:\n",
            " Current_Weight_Tons: 0.00% пропусков\n",
            " Stack_Age_Days: 3.12% пропусков\n",
            " Days_Since_Last_Measurement: 75.03% пропусков\n",
            "\n",
            "--- Сохранение мастер-сетки с динамическими признаками ---\n",
            "Мастер-сетка с динамическими признаками успешно сохранена в файл: processed_data_final/master_grid_with_dynamic_features.csv\n",
            "\n",
            "--- Шаг 8 ЗАВЕРШЕН ---\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"\\n--- Шаг 9: Добавление целевой переменной (Fire_Started_Today) ---\")\n",
        "\n",
        "# Проверяем наличие необходимых DataFrame'ов\n",
        "if 'master_grid' not in locals() or master_grid.empty or \\\n",
        "   'df_fires_a1' not in locals() or df_fires_a1.empty:\n",
        "      print(\"Ошибка: Отсутствует master_grid или df_fires_a1. Добавление целевой переменной невозможно.\")\n",
        "else:\n",
        "    # 9.1 Подготовка df_fires_a1\n",
        "    print(\"Подготовка данных о возгораниях (df_fires_a1)...\")\n",
        "    fires_to_merge = df_fires_a1[['Склад', 'Штабель', 'Дата начала']].copy()\n",
        "\n",
        "    # Убедимся, что 'Дата начала' - datetime\n",
        "    if not pd.api.types.is_datetime64_any_dtype(fires_to_merge['Дата начала']):\n",
        "        fires_to_merge['Дата начала'] = pd.to_datetime(fires_to_merge['Дата начала'], errors='coerce')\n",
        "\n",
        "    # Извлекаем только дату (без времени)\n",
        "    fires_to_merge['Дата'] = fires_to_merge['Дата начала'].dt.normalize()\n",
        "\n",
        "    # Приводим ключи к строкам\n",
        "    fires_to_merge['Склад'] = fires_to_merge['Склад'].astype(str)\n",
        "    fires_to_merge['Штабель'] = fires_to_merge['Штабель'].astype(str)\n",
        "\n",
        "    # Удаляем строки с некорректной датой начала\n",
        "    initial_fires_rows = len(fires_to_merge)\n",
        "    fires_to_merge.dropna(subset=['Склад', 'Штабель', 'Дата'], inplace=True)\n",
        "    if len(fires_to_merge) < initial_fires_rows:\n",
        "         print(f\" Удалено {initial_fires_rows - len(fires_to_merge)} строк с NaN в ключах/дате из данных о пожарах.\")\n",
        "\n",
        "    # Создаем колонку-индикатор пожара\n",
        "    fires_to_merge['Fire_Started_Today'] = 1\n",
        "    print(f\" Найдено записей о начале пожаров: {len(fires_to_merge)}\")\n",
        "\n",
        "    # Обработка дубликатов (если вдруг в один день для одного штабеля >1 записи о начале пожара)\n",
        "    # Оставляем только одну запись (max() на индикаторе даст 1)\n",
        "    fires_agg = fires_to_merge.groupby(['Склад', 'Штабель', 'Дата'], as_index=False)['Fire_Started_Today'].max()\n",
        "    print(f\" Уникальных событий начала пожара (Склад/Штабель/День): {len(fires_agg)}\")\n",
        "\n",
        "    # 9.2 Подготовка master_grid (типы ключей уже должны быть проверены)\n",
        "    print(\"\\nПодготовка master_grid к слиянию (проверка типов ключей)...\")\n",
        "    if not pd.api.types.is_string_dtype(master_grid['Склад']): master_grid['Склад'] = master_grid['Склад'].astype(str)\n",
        "    if not pd.api.types.is_string_dtype(master_grid['Штабель']): master_grid['Штабель'] = master_grid['Штабель'].astype(str)\n",
        "    if not pd.api.types.is_datetime64_any_dtype(master_grid['Дата']): master_grid['Дата'] = pd.to_datetime(master_grid['Дата'], errors='coerce')\n",
        "    master_grid.dropna(subset=['Дата'], inplace=True) # На всякий случай\n",
        "    print(\" Типы ключей в master_grid проверены.\")\n",
        "\n",
        "\n",
        "    # 9.3 Слияние с данными о пожарах\n",
        "    print(\"\\nВыполнение слияния master_grid с данными о пожарах...\")\n",
        "    master_grid = pd.merge(\n",
        "        master_grid,\n",
        "        fires_agg[['Склад', 'Штабель', 'Дата', 'Fire_Started_Today']],\n",
        "        on=['Склад', 'Штабель', 'Дата'],\n",
        "        how='left'\n",
        "    )\n",
        "    print(f\" Размер сетки после слияния с данными о пожарах: {master_grid.shape}\")\n",
        "    print(f\" Колонки в master_grid: {master_grid.columns.tolist()}\")\n",
        "\n",
        "\n",
        "    # 9.4 Заполнение NaN нулями в целевой переменной\n",
        "    print(\"\\nЗаполнение NaN в 'Fire_Started_Today' нулями...\")\n",
        "    if 'Fire_Started_Today' in master_grid.columns:\n",
        "        master_grid['Fire_Started_Today'] = master_grid['Fire_Started_Today'].fillna(0).astype(int) # Заполняем 0 и преобразуем в int\n",
        "        print(\" NaN в Fire_Started_Today заменены на 0.\")\n",
        "        # Проверим, сколько всего пожаров в итоговой сетке\n",
        "        total_fires_in_grid = master_grid['Fire_Started_Today'].sum()\n",
        "        print(f\" Общее количество дней с началом пожара в мастер-сетке: {total_fires_in_grid}\")\n",
        "        # Сравним с количеством событий в fires_agg - должно совпадать\n",
        "        if total_fires_in_grid == len(fires_agg):\n",
        "            print(\" Количество совпадает с исходными агрегированными данными о пожарах. ОК.\")\n",
        "        else:\n",
        "            print(f\" ПРЕДУПРЕЖДЕНИЕ: Количество пожаров не совпадает! В сетке: {total_fires_in_grid}, в исходных: {len(fires_agg)}\")\n",
        "    else:\n",
        "        print(\" ОШИБКА: Колонка 'Fire_Started_Today' не найдена после слияния!\")\n",
        "\n",
        "    # 9.5 Сохранение результата (опционально, но полезно)\n",
        "    save_final_grid = True\n",
        "    if save_final_grid:\n",
        "        print(\"\\n--- Сохранение финальной мастер-сетки с целевой переменной ---\")\n",
        "        output_dir_final = \"processed_data_final\"\n",
        "        if not os.path.exists(output_dir_final):\n",
        "            os.makedirs(output_dir_final)\n",
        "        output_path_final = os.path.join(output_dir_final, 'master_grid_final_with_target.csv')\n",
        "        try:\n",
        "            master_grid.to_csv(output_path_final, index=False)\n",
        "            print(f\"Финальная мастер-сетка успешно сохранена в файл: {output_path_final}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при сохранении файла: {e}\")\n",
        "\n",
        "print(\"\\n--- Шаг 9 ЗАВЕРШЕН ---\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCq8T_md77SJ",
        "outputId": "c5acb979-e703-48ff-ff04-c9ec4bbb769e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Шаг 9: Добавление целевой переменной (Fire_Started_Today) ---\n",
            "Ошибка: Отсутствует master_grid или df_fires_a1. Добавление целевой переменной невозможно.\n",
            "\n",
            "--- Шаг 9 ЗАВЕРШЕН ---\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Откат к состоянию после Шага 9 ---\n",
        "print(\"--- Откат к состоянию DataFrame после Шага 9 ---\")\n",
        "print(\"(Включая погоду, температуру, поставки, базовые динамические признаки и ОРИГИНАЛЬНУЮ бинарную цель)\")\n",
        "\n",
        "output_dir_final = \"processed_data_final\"\n",
        "# Имя файла, сохраненного в конце Шага 9\n",
        "input_file_step9 = os.path.join(output_dir_final, 'master_grid_with_dynamic_features.csv')\n",
        "\n",
        "# Попытка загрузить\n",
        "if os.path.exists(input_file_step9):\n",
        "    print(f\"Загрузка данных из файла: {input_file_step9}...\")\n",
        "    try:\n",
        "        # Загружаем данные, парсим дату\n",
        "        df_reverted = pd.read_csv(input_file_step9, parse_dates=['Дата'])\n",
        "\n",
        "        # Переименовываем DataFrame для дальнейшей работы (например, обратно в df_eda)\n",
        "        df_eda = df_reverted.copy()\n",
        "        print(\"Данные успешно загружены и скопированы в df_eda.\")\n",
        "        print(f\"Размер DataFrame: {df_eda.shape}\")\n",
        "        print(\"\\nКолонки DataFrame:\")\n",
        "        print(df_eda.columns.tolist())\n",
        "        print(\"\\nИнформация о типах данных:\")\n",
        "        df_eda.info()\n",
        "        print(\"\\nПример данных:\")\n",
        "        print(df_eda.head())\n",
        "        print(\"\\nПроверка целевой переменной (должна быть 0/1):\")\n",
        "        print(df_eda['Fire_Started_Today'].value_counts())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке или обработке файла {input_file_step9}: {e}\")\n",
        "        print(\"Не удалось откатиться к нужному состоянию.\")\n",
        "        # Оставляем df_eda как есть или обнуляем, в зависимости от ситуации\n",
        "        # df_eda = pd.DataFrame() # Раскомментируй, если нужно обнулить в случае ошибки\n",
        "else:\n",
        "    print(f\"Ошибка: Файл {input_file_step9} не найден.\")\n",
        "    print(\"Не удалось откатиться к нужному состоянию.\")\n",
        "    print(\"Убедитесь, что Шаг 9 был выполнен и файл был сохранен с этим именем.\")\n",
        "    # Возможно, нужно будет перезапустить предыдущие шаги для генерации этого файла\n",
        "\n",
        "print(\"\\n--- Откат завершен (или не удался) ---\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTsOAcSNjXpW",
        "outputId": "87c6b915-60f5-491b-a0fb-14ea73a39d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Откат к состоянию DataFrame после Шага 9 ---\n",
            "(Включая погоду, температуру, поставки, базовые динамические признаки и ОРИГИНАЛЬНУЮ бинарную цель)\n",
            "Загрузка данных из файла: processed_data_final/master_grid_with_dynamic_features.csv...\n",
            "Данные успешно загружены и скопированы в df_eda.\n",
            "Размер DataFrame: (70176, 22)\n",
            "\n",
            "Колонки DataFrame:\n",
            "['Склад', 'Штабель', 'Дата', 't_mean', 't_min', 't_max', 'p_mean', 'p_min', 'p_max', 'humidity_mean', 'humidity_min', 'precip_total_day', 'cloudcover_mean', 'weather_code_mode', 'Temp_Measure_Max', 'Added_Today_Tons', 'Removed_Today_Tons', 'Daily_Weight_Change', 'Current_Weight_Tons', 'Formation_Date', 'Stack_Age_Days', 'Days_Since_Last_Measurement']\n",
            "\n",
            "Информация о типах данных:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 70176 entries, 0 to 70175\n",
            "Data columns (total 22 columns):\n",
            " #   Column                       Non-Null Count  Dtype         \n",
            "---  ------                       --------------  -----         \n",
            " 0   Склад                        70176 non-null  int64         \n",
            " 1   Штабель                      70176 non-null  int64         \n",
            " 2   Дата                         70176 non-null  datetime64[ns]\n",
            " 3   t_mean                       35040 non-null  float64       \n",
            " 4   t_min                        35040 non-null  float64       \n",
            " 5   t_max                        35040 non-null  float64       \n",
            " 6   p_mean                       35040 non-null  float64       \n",
            " 7   p_min                        35040 non-null  float64       \n",
            " 8   p_max                        35040 non-null  float64       \n",
            " 9   humidity_mean                35040 non-null  float64       \n",
            " 10  humidity_min                 35040 non-null  float64       \n",
            " 11  precip_total_day             35040 non-null  float64       \n",
            " 12  cloudcover_mean              35040 non-null  float64       \n",
            " 13  weather_code_mode            35040 non-null  float64       \n",
            " 14  Temp_Measure_Max             2175 non-null   float64       \n",
            " 15  Added_Today_Tons             70176 non-null  float64       \n",
            " 16  Removed_Today_Tons           70176 non-null  float64       \n",
            " 17  Daily_Weight_Change          70176 non-null  float64       \n",
            " 18  Current_Weight_Tons          70176 non-null  float64       \n",
            " 19  Formation_Date               67983 non-null  object        \n",
            " 20  Stack_Age_Days               67983 non-null  float64       \n",
            " 21  Days_Since_Last_Measurement  17526 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(18), int64(2), object(1)\n",
            "memory usage: 11.8+ MB\n",
            "\n",
            "Пример данных:\n",
            "   Склад  Штабель       Дата  t_mean  t_min  t_max  p_mean  p_min  p_max  \\\n",
            "0      3        0 2019-01-01     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "1      3        0 2019-01-02     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "2      3        0 2019-01-03     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "3      3        0 2019-01-04     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "4      3        0 2019-01-05     NaN    NaN    NaN     NaN    NaN    NaN   \n",
            "\n",
            "   humidity_mean  ...  cloudcover_mean  weather_code_mode  Temp_Measure_Max  \\\n",
            "0            NaN  ...              NaN                NaN               NaN   \n",
            "1            NaN  ...              NaN                NaN               NaN   \n",
            "2            NaN  ...              NaN                NaN               NaN   \n",
            "3            NaN  ...              NaN                NaN               NaN   \n",
            "4            NaN  ...              NaN                NaN               NaN   \n",
            "\n",
            "   Added_Today_Tons  Removed_Today_Tons  Daily_Weight_Change  \\\n",
            "0               0.0                 0.0                  0.0   \n",
            "1               0.0                 0.0                  0.0   \n",
            "2               0.0                 0.0                  0.0   \n",
            "3               0.0                 0.0                  0.0   \n",
            "4               0.0                 0.0                  0.0   \n",
            "\n",
            "   Current_Weight_Tons  Formation_Date  Stack_Age_Days  \\\n",
            "0                  0.0             NaN             NaN   \n",
            "1                  0.0             NaN             NaN   \n",
            "2                  0.0             NaN             NaN   \n",
            "3                  0.0             NaN             NaN   \n",
            "4                  0.0             NaN             NaN   \n",
            "\n",
            "  Days_Since_Last_Measurement  \n",
            "0                         NaN  \n",
            "1                         NaN  \n",
            "2                         NaN  \n",
            "3                         NaN  \n",
            "4                         NaN  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "\n",
            "Проверка целевой переменной (должна быть 0/1):\n",
            "Ошибка при загрузке или обработке файла processed_data_final/master_grid_with_dynamic_features.csv: 'Fire_Started_Today'\n",
            "Не удалось откатиться к нужному состоянию.\n",
            "\n",
            "--- Откат завершен (или не удался) ---\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"\\n--- Шаг 11 (Альтернативный): Фильтрация DataFrame по замерам температуры и периодам перед пожарами ---\")\n",
        "\n",
        "# --- 11.1 Загрузка необходимых данных ---\n",
        "\n",
        "# Загрузка основного DataFrame (состояние после Шага 9)\n",
        "output_dir_final = \"processed_data_final\"\n",
        "input_file_main = os.path.join(output_dir_final, 'master_grid_with_dynamic_features.csv') # Файл с бинарной целью\n",
        "print(f\"Загрузка основного DataFrame из {input_file_main}...\")\n",
        "if os.path.exists(input_file_main):\n",
        "    df_main = pd.read_csv(input_file_main, parse_dates=['Дата'])\n",
        "    print(f\"Основной DataFrame загружен. Размер: {df_main.shape}\")\n",
        "else:\n",
        "    print(f\"Ошибка: Файл {input_file_main} не найден. Невозможно продолжить.\")\n",
        "    raise FileNotFoundError(f\"Файл {input_file_main} не найден.\")\n",
        "\n",
        "# Загрузка данных о температуре (обработанных ранее)\n",
        "input_file_temp = os.path.join(\"processed_data_typed\", 'temperature_A1_typed.csv') # Файл с обработанными типами\n",
        "# ИЛИ если сохраняли на другом шаге: os.path.join(output_dir_final, 'temperature_A1_processed.csv')\n",
        "print(f\"Загрузка данных о температуре из {input_file_temp}...\")\n",
        "if os.path.exists(input_file_temp):\n",
        "    df_temp_source = pd.read_csv(input_file_temp, parse_dates=['Дата акта']) # Убедимся, что Дата акта парсится\n",
        "    print(f\"Данные о температуре загружены. Размер: {df_temp_source.shape}\")\n",
        "else:\n",
        "    # Попробуем другой возможный путь\n",
        "    input_file_temp_alt = os.path.join(\"processed_data\", 'temperature_A1_processed.csv')\n",
        "    if os.path.exists(input_file_temp_alt):\n",
        "         print(f\"Загрузка данных о температуре из альтернативного пути: {input_file_temp_alt}...\")\n",
        "         df_temp_source = pd.read_csv(input_file_temp_alt)\n",
        "         # Преобразуем дату акта, если нужно\n",
        "         if 'Дата акта' in df_temp_source.columns and not pd.api.types.is_datetime64_any_dtype(df_temp_source['Дата акта']):\n",
        "              df_temp_source['Дата акта'] = pd.to_datetime(df_temp_source['Дата акта'], errors='coerce')\n",
        "         print(f\"Данные о температуре загружены. Размер: {df_temp_source.shape}\")\n",
        "    else:\n",
        "        print(f\"Ошибка: Файл данных о температуре не найден ни по одному из путей ({input_file_temp}, {input_file_temp_alt}). Невозможно продолжить.\")\n",
        "        raise FileNotFoundError(\"Файл данных о температуре не найден.\")\n",
        "\n",
        "\n",
        "# --- 11.2 Подготовка ключей и типов ---\n",
        "print(\"\\nПодготовка ключей и типов данных...\")\n",
        "\n",
        "# Преобразуем ключи в строки для надежности сравнения/объединения\n",
        "df_main['Склад'] = df_main['Склад'].astype(str)\n",
        "df_main['Штабель'] = df_main['Штабель'].astype(str)\n",
        "df_temp_source['Склад'] = df_temp_source['Склад'].astype(str)\n",
        "df_temp_source['Штабель'] = df_temp_source['Штабель'].astype(str)\n",
        "\n",
        "# Извлекаем дату из 'Дата акта'\n",
        "if 'Дата акта' in df_temp_source.columns and pd.api.types.is_datetime64_any_dtype(df_temp_source['Дата акта']):\n",
        "    df_temp_source['Дата'] = df_temp_source['Дата акта'].dt.normalize()\n",
        "    # Удаляем строки с некорректной датой, если они появились\n",
        "    df_temp_source.dropna(subset=['Дата'], inplace=True)\n",
        "else:\n",
        "    print(\"Ошибка: Колонка 'Дата акта' отсутствует или не является datetime в данных температуры.\")\n",
        "    raise TypeError(\"Необходимая колонка 'Дата акта' не найдена или имеет неверный тип.\")\n",
        "\n",
        "# Убедимся, что 'Дата' в df_main тоже datetime (должна быть после parse_dates)\n",
        "if not pd.api.types.is_datetime64_any_dtype(df_main['Дата']):\n",
        "     df_main['Дата'] = pd.to_datetime(df_main['Дата'], errors='coerce')\n",
        "     df_main.dropna(subset=['Дата'], inplace=True)\n",
        "\n",
        "print(\"Ключи и типы подготовлены.\")\n",
        "\n",
        "\n",
        "# --- 11.3 Определение ключей по замерам температуры (Set 1) ---\n",
        "print(\"\\nОпределение ключей по дням замеров температуры...\")\n",
        "# Выбираем только уникальные комбинации Склад/Штабель/Дата из данных температуры\n",
        "temp_keys_df = df_temp_source[['Склад', 'Штабель', 'Дата']].drop_duplicates()\n",
        "print(f\"Найдено уникальных ключей (Склад, Штабель, Дата) с замерами: {len(temp_keys_df)}\")\n",
        "\n",
        "\n",
        "# --- 11.4 Определение ключей по пожарам и 5 дням до (Set 2) ---\n",
        "print(\"\\nОпределение ключей по дням пожаров и 5 предшествующим дням...\")\n",
        "target_col = 'Fire_Started_Today'\n",
        "days_before = 5\n",
        "\n",
        "# Находим все события пожара\n",
        "# fire_events_df = df_main[df_main[target_col] == 1][['Склад', 'Штабель', 'Дата']].copy()\n",
        "# print(f\"Найдено событий пожара: {len(fire_events_df)}\")\n",
        "\n",
        "# Собираем ключи в DataFrame для удобства\n",
        "# fire_related_keys_list = []\n",
        "# if not fire_events_df.empty:\n",
        "#     for index, fire_event in tqdm(fire_events_df.iterrows(), total=len(fire_events_df), desc=\"Обработка пожаров\"):\n",
        "#         sklad = fire_event['Склад']\n",
        "#         shtabel = fire_event['Штабель']\n",
        "#         fire_date = fire_event['Дата']\n",
        "\n",
        "#         # Добавляем ключи для дня пожара и 5 дней до\n",
        "#         for k in range(days_before + 1): # от 0 до 5 включительно\n",
        "#             target_date = fire_date - pd.Timedelta(days=k)\n",
        "#             fire_related_keys_list.append({'Склад': sklad, 'Штабель': shtabel, 'Дата': target_date})\n",
        "\n",
        "#     # Создаем DataFrame из списка и удаляем дубликаты\n",
        "#     fire_keys_df = pd.DataFrame(fire_related_keys_list).drop_duplicates()\n",
        "#     print(f\"Найдено уникальных ключей, связанных с пожарами (+{days_before} дней до): {len(fire_keys_df)}\")\n",
        "# else:\n",
        "#     print(\"Событий пожара не найдено, набор ключей пуст.\")\n",
        "#     fire_keys_df = pd.DataFrame(columns=['Склад', 'Штабель', 'Дата'])\n",
        "\n",
        "\n",
        "# --- 11.5 Объединение ключей ---\n",
        "print(\"\\nОбъединение наборов ключей...\")\n",
        "# Объединяем два DataFrame с ключами\n",
        "# combined_keys_df = pd.concat([temp_keys_df, fire_keys_df], ignore_index=True)\n",
        "# Удаляем дубликаты, если день замера совпал с днем перед пожаром\n",
        "# combined_keys_df.drop_duplicates(inplace=True)\n",
        "# print(f\"Общее количество уникальных ключей для сохранения: {len(combined_keys_df)}\")\n",
        "\n",
        "\n",
        "# --- 11.6 Фильтрация основного DataFrame ---\n",
        "print(\"\\nФильтрация основного DataFrame по объединенным ключам...\")\n",
        "# Используем inner merge для выполнения \"полусоединения\" (semi-join)\n",
        "# Это оставит только те строки из df_main, ключи которых есть в combined_keys_df\n",
        "# df_filtered = pd.merge(\n",
        "#     df_main,\n",
        "#     combined_keys_df,\n",
        "#     on=['Склад', 'Штабель', 'Дата'],\n",
        "#     how='inner' # Оставляем только совпадающие строки\n",
        "# )\n",
        "\n",
        "# print(f\"\\nРазмер основного DataFrame *до* фильтрации: {df_main.shape}\")\n",
        "# print(f\"Размер основного DataFrame *после* фильтрации: {df_filtered.shape}\")\n",
        "\n",
        "# # --- 11.7 Результат ---\n",
        "# print(\"\\nФильтрация завершена.\")\n",
        "# # Переименовываем результат для ясности\n",
        "# df_eda_filtered = df_filtered.copy()\n",
        "print(\"\\nПример отфильтрованных данных:\")\n",
        "print(df_eda_filtered.head())\n",
        "print(\"\\nПроверка целевой переменной в отфильтрованных данных:\")\n",
        "print(df_eda_filtered[target_col].value_counts())\n",
        "\n",
        "# Опционально: Сохранение результата\n",
        "save_filtered_df = True\n",
        "if save_filtered_df:\n",
        "    output_file_filtered = os.path.join(output_dir_final, 'master_grid_filtered_temp_fireperiod.csv')\n",
        "    print(f\"\\nСохранение отфильтрованного DataFrame в {output_file_filtered}...\")\n",
        "    try:\n",
        "        df_eda_filtered.to_csv(output_file_filtered, index=False)\n",
        "        print(\"Файл успешно сохранен.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при сохранении файла: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Шаг 11 (Альтернативный) ЗАВЕРШЕН ---\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "JWGx9FWljfEf",
        "outputId": "55cbfb6f-3678-4cd4-d6f0-412a6ac4bc87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Шаг 11 (Альтернативный): Фильтрация DataFrame по замерам температуры и периодам перед пожарами ---\n",
            "Загрузка основного DataFrame из processed_data_final/master_grid_with_dynamic_features.csv...\n",
            "Основной DataFrame загружен. Размер: (70176, 22)\n",
            "Загрузка данных о температуре из processed_data_typed/temperature_A1_typed.csv...\n",
            "Данные о температуре загружены. Размер: (4095, 5)\n",
            "\n",
            "Подготовка ключей и типов данных...\n",
            "Ключи и типы подготовлены.\n",
            "\n",
            "Определение ключей по дням замеров температуры...\n",
            "Найдено уникальных ключей (Склад, Штабель, Дата) с замерами: 2175\n",
            "\n",
            "Определение ключей по дням пожаров и 5 предшествующим дням...\n",
            "\n",
            "Объединение наборов ключей...\n",
            "\n",
            "Фильтрация основного DataFrame по объединенным ключам...\n",
            "\n",
            "Пример отфильтрованных данных:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_eda_filtered' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6d4d58cd9928>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m# df_eda_filtered = df_filtered.copy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nПример отфильтрованных данных:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_eda_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nПроверка целевой переменной в отфильтрованных данных:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_eda_filtered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_eda_filtered' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "NGWVaUpI1UFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Шаг Y: Загрузка, Очистка и Сохранение CSV файла ---\n",
        "print(\"\\n--- Загрузка, Очистка и Сохранение CSV файла ---\")\n",
        "\n",
        "# 1. Укажи пути и имена файлов\n",
        "input_directory = \".\"  # Укажи папку, где лежит файл (или \".\" для текущей)\n",
        "input_filename = \"master_grid_filtered_temp_fireperiod.csv\" # Имя твоего файла\n",
        "output_directory = \"processed_data_final\" # Папка для сохранения результата\n",
        "output_filename = \"master_grid_cleaned.csv\" # Имя для очищенного файла\n",
        "\n",
        "input_filepath = os.path.join(input_directory, input_filename)\n",
        "output_filepath = os.path.join(output_directory, output_filename)\n",
        "\n",
        "# 2. Список столбцов для удаления\n",
        "cols_to_drop = [\n",
        "    't_min',\n",
        "    't_max',\n",
        "    'p_min',\n",
        "    'p_max',\n",
        "    'humidity_min',\n",
        "    'weather_code_mode'\n",
        "    # Добавь сюда другие колонки, если нужно\n",
        "]\n",
        "print(f\"Будут удалены столбцы (если они есть): {cols_to_drop}\")\n",
        "\n",
        "# 3. Проверка существования входного файла и загрузка\n",
        "if os.path.exists(input_filepath):\n",
        "    print(f\"\\nЗагрузка данных из файла: {input_filepath}...\")\n",
        "    try:\n",
        "        # Загружаем DataFrame\n",
        "        df_input = pd.read_csv(input_filepath)\n",
        "        print(f\"Файл успешно загружен. Размер: {df_input.shape}\")\n",
        "        print(f\"Исходные колонки: {df_input.columns.tolist()}\")\n",
        "\n",
        "        # 4. Удаление столбцов\n",
        "        # Находим столбцы, которые реально существуют\n",
        "        existing_cols_to_drop = [col for col in cols_to_drop if col in df_input.columns]\n",
        "\n",
        "        if existing_cols_to_drop:\n",
        "            print(f\"\\nУдаление столбцов: {existing_cols_to_drop}...\")\n",
        "            try:\n",
        "                # Удаляем столбцы\n",
        "                df_cleaned = df_input.drop(columns=existing_cols_to_drop, errors='ignore')\n",
        "                print(\"Столбцы успешно удалены.\")\n",
        "                print(f\"Новое количество столбцов: {df_cleaned.shape[1]}\")\n",
        "                print(f\"Оставшиеся колонки: {df_cleaned.columns.tolist()}\")\n",
        "\n",
        "                # 5. Сохранение результата в новый файл\n",
        "                # Убедимся, что папка для вывода существует\n",
        "                if not os.path.exists(output_directory):\n",
        "                    os.makedirs(output_directory)\n",
        "                    print(f\"\\nСоздана директория для вывода: {output_directory}\")\n",
        "\n",
        "                print(f\"\\nСохранение очищенного DataFrame в файл: {output_filepath}...\")\n",
        "                df_cleaned.to_csv(output_filepath, index=False)\n",
        "                print(\"Файл успешно сохранен.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при удалении столбцов или сохранении файла: {e}\")\n",
        "        else:\n",
        "            print(\"\\nНи один из указанных столбцов для удаления не найден в DataFrame. Файл не изменен.\")\n",
        "            # Можно добавить сохранение исходного файла под новым именем, если нужно\n",
        "            # print(f\"\\nКопирование исходного файла в: {output_filepath}...\")\n",
        "            # df_input.to_csv(output_filepath, index=False)\n",
        "            # print(\"Файл скопирован без изменений.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке или обработке файла {input_filepath}: {e}\")\n",
        "else:\n",
        "    print(f\"Ошибка: Входной файл не найден по пути: {input_filepath}\")\n",
        "\n",
        "print(\"\\n--- Обработка файла завершена ---\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "35h_tc_CNvJV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}